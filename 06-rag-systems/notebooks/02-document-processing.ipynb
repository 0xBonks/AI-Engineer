{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06 - Notebook 02: Document Processing Pipeline\n",
    "\n",
    "## Learning Objectives\n",
    "- Load documents from various formats (PDF, DOCX, TXT, HTML)\n",
    "- Implement smart chunking strategies\n",
    "- Extract and preserve metadata\n",
    "- Clean and preprocess text\n",
    "- Build a complete ingestion pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Loading\n",
    "\n",
    "RAG systems need to handle multiple document formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q PyPDF2 python-docx beautifulsoup4 langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    "    UnstructuredHTMLLoader\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from pathlib import Path\n",
    "\n",
    "class DocumentLoader:\n",
    "    \"\"\"Load documents from various formats.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_file(filepath: str) -> list[Document]:\n",
    "        \"\"\"Auto-detect format and load.\"\"\"\n",
    "        path = Path(filepath)\n",
    "        suffix = path.suffix.lower()\n",
    "        \n",
    "        loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.pdf': PyPDFLoader,\n",
    "            '.docx': Docx2txtLoader,\n",
    "            '.html': UnstructuredHTMLLoader\n",
    "        }\n",
    "        \n",
    "        loader_class = loaders.get(suffix)\n",
    "        if not loader_class:\n",
    "            raise ValueError(f\"Unsupported file type: {suffix}\")\n",
    "        \n",
    "        loader = loader_class(filepath)\n",
    "        return loader.load()\n",
    "\n",
    "# Example: Create a sample text file\n",
    "sample_text = \"\"\"This is a sample document for RAG.\n",
    "It contains multiple paragraphs.\n",
    "\n",
    "This is the second paragraph with more information.\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"/tmp/sample.txt\", \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Load it\n",
    "docs = DocumentLoader.load_file(\"/tmp/sample.txt\")\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Content: {docs[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Smart Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents: list[Document], chunk_size: int = 500) -> list[Document]:\n",
    "    \"\"\"Chunk documents with optimal settings.\"\"\"\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1),  # 10% overlap\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    # Add chunk metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"chunk_id\"] = i\n",
    "        chunk.metadata[\"chunk_size\"] = len(chunk.page_content)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "long_doc = Document(\n",
    "    page_content=\"This is a long document. \" * 100,\n",
    "    metadata={\"source\": \"test.txt\"}\n",
    ")\n",
    "\n",
    "chunks = chunk_documents([long_doc], chunk_size=200)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk:\")\n",
    "print(f\"  Content: {chunks[0].page_content[:100]}...\")\n",
    "print(f\"  Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class MetadataExtractor:\n",
    "    \"\"\"Extract metadata from documents.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_from_text(text: str) -> dict:\n",
    "        \"\"\"Extract metadata like dates, emails, URLs.\"\"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        # Extract emails\n",
    "        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "        if emails:\n",
    "            metadata['emails'] = emails\n",
    "        \n",
    "        # Extract URLs\n",
    "        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "        if urls:\n",
    "            metadata['urls'] = urls\n",
    "        \n",
    "        # Extract dates (simple pattern)\n",
    "        dates = re.findall(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', text)\n",
    "        if dates:\n",
    "            metadata['dates'] = dates\n",
    "        \n",
    "        # Basic stats\n",
    "        metadata['word_count'] = len(text.split())\n",
    "        metadata['char_count'] = len(text)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    @staticmethod\n",
    "    def enrich_document(doc: Document) -> Document:\n",
    "        \"\"\"Add extracted metadata to document.\"\"\"\n",
    "        extracted = MetadataExtractor.extract_from_text(doc.page_content)\n",
    "        doc.metadata.update(extracted)\n",
    "        return doc\n",
    "\n",
    "# Test metadata extraction\n",
    "test_doc = Document(\n",
    "    page_content=\"\"\"Contact us at support@company.com\n",
    "    Visit https://example.com for more info.\n",
    "    Last updated: 2024-01-15\"\"\",\n",
    "    metadata={\"source\": \"contact.txt\"}\n",
    ")\n",
    "\n",
    "enriched = MetadataExtractor.enrich_document(test_doc)\n",
    "print(\"Enriched metadata:\")\n",
    "print(enriched.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "\n",
    "class IngestionPipeline:\n",
    "    \"\"\"Complete pipeline: Load â†’ Clean â†’ Chunk â†’ Embed â†’ Store.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str):\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.collection = self.chroma_client.create_collection(collection_name)\n",
    "    \n",
    "    def process_file(self, filepath: str, chunk_size: int = 500):\n",
    "        \"\"\"Complete pipeline for a single file.\"\"\"\n",
    "        print(f\"Processing: {filepath}\")\n",
    "        \n",
    "        # Step 1: Load\n",
    "        docs = DocumentLoader.load_file(filepath)\n",
    "        print(f\"  Loaded: {len(docs)} documents\")\n",
    "        \n",
    "        # Step 2: Enrich metadata\n",
    "        docs = [MetadataExtractor.enrich_document(doc) for doc in docs]\n",
    "        print(f\"  Enriched metadata\")\n",
    "        \n",
    "        # Step 3: Chunk\n",
    "        chunks = chunk_documents(docs, chunk_size)\n",
    "        print(f\"  Created: {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 4: Embed\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        response = self.openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=texts\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        print(f\"  Generated: {len(embeddings)} embeddings\")\n",
    "        \n",
    "        # Step 5: Store\n",
    "        self.collection.add(\n",
    "            documents=texts,\n",
    "            embeddings=embeddings,\n",
    "            metadatas=[chunk.metadata for chunk in chunks],\n",
    "            ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "        )\n",
    "        print(f\"  Stored in vector database\")\n",
    "        print(f\"âœ“ Pipeline complete: {self.collection.count()} total chunks\\n\")\n",
    "\n",
    "# Demo the pipeline\n",
    "pipeline = IngestionPipeline(\"rag_demo\")\n",
    "pipeline.process_file(\"/tmp/sample.txt\", chunk_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Loading multiple document formats\n",
    "- âœ… Smart chunking strategies\n",
    "- âœ… Metadata extraction and enrichment\n",
    "- âœ… Building complete ingestion pipelines\n",
    "\n",
    "## Best Practices\n",
    "1. **Auto-detect** file formats\n",
    "2. **Preserve metadata** through processing\n",
    "3. **Use recursive chunking** for best results\n",
    "4. **Extract structured info** (dates, emails, URLs)\n",
    "5. **Batch embed** for efficiency\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 03: Retrieval Strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
