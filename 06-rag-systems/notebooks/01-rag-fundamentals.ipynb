{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06 - Notebook 01: RAG Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what RAG (Retrieval-Augmented Generation) is\n",
    "- Compare RAG vs fine-tuning\n",
    "- Learn RAG architecture and components\n",
    "- Build your first simple RAG system\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines:\n",
    "- **Retrieval**: Finding relevant information from a knowledge base\n",
    "- **Generation**: Using an LLM to generate answers based on retrieved context\n",
    "\n",
    "### The RAG Pipeline:\n",
    "```\n",
    "User Query\n",
    "    â†“\n",
    "1. Retrieve relevant documents (vector search)\n",
    "    â†“\n",
    "2. Construct prompt with context\n",
    "    â†“\n",
    "3. Generate answer with LLM\n",
    "    â†“\n",
    "Answer + Sources\n",
    "```\n",
    "\n",
    "### Why RAG?\n",
    "- âœ… Use **your own data** without retraining\n",
    "- âœ… **Up-to-date** information (just update the knowledge base)\n",
    "- âœ… **Citeable sources** for transparency\n",
    "- âœ… **Lower cost** than fine-tuning\n",
    "- âœ… **Quick to implement** and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG vs Fine-Tuning\n",
    "\n",
    "| Aspect | RAG | Fine-Tuning |\n",
    "|--------|-----|-------------|\n",
    "| **Setup Time** | Hours | Days/Weeks |\n",
    "| **Cost** | Low (API + storage) | High (training compute) |\n",
    "| **Updates** | Add to knowledge base | Retrain model |\n",
    "| **Sources** | Citable | No citations |\n",
    "| **Use Case** | Q&A, documentation, support | Style, tone, specialized tasks |\n",
    "| **Flexibility** | Very flexible | Fixed after training |\n",
    "\n",
    "### When to Use RAG:\n",
    "- Frequently changing information\n",
    "- Need source citations\n",
    "- Multiple knowledge domains\n",
    "- Quick deployment needed\n",
    "\n",
    "### When to Fine-Tune:\n",
    "- Specific writing style needed\n",
    "- Domain-specific language\n",
    "- Lower latency required\n",
    "- Static knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai chromadb sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chroma_client = chromadb.Client()\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"âœ“ All systems ready for RAG!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Your First RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"A minimal RAG system to understand the basics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.collection = self.chroma_client.create_collection(\"simple_rag\")\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def add_documents(self, documents: list):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(documents).tolist()\n",
    "        \n",
    "        # Store in vector DB\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Added {len(documents)} documents\")\n",
    "    \n",
    "    def retrieve(self, query: str, n_results: int = 3) -> list:\n",
    "        \"\"\"Retrieve relevant documents.\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        return results['documents'][0]\n",
    "    \n",
    "    def generate(self, query: str, context: list) -> str:\n",
    "        \"\"\"Generate answer using retrieved context.\"\"\"\n",
    "        # Construct prompt\n",
    "        context_text = \"\\n\\n\".join(context)\n",
    "        \n",
    "        prompt = f\"\"\"Answer the question based on the context below. If the answer is not in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Generate with LLM\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Complete RAG: retrieve + generate.\"\"\"\n",
    "        # Step 1: Retrieve\n",
    "        context = self.retrieve(question)\n",
    "        \n",
    "        # Step 2: Generate\n",
    "        answer = self.generate(question, context)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": context\n",
    "        }\n",
    "\n",
    "# Demo the RAG system\n",
    "rag = SimpleRAG()\n",
    "\n",
    "# Add knowledge base\n",
    "knowledge = [\n",
    "    \"Python was created by Guido van Rossum and released in 1991.\",\n",
    "    \"Python is known for its simple and readable syntax.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are inspired by biological neurons in the brain.\",\n",
    "    \"The pandas library is used for data analysis in Python.\",\n",
    "    \"TensorFlow and PyTorch are popular deep learning frameworks.\"\n",
    "]\n",
    "\n",
    "rag.add_documents(knowledge)\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What is the capital of France?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "for q in questions:\n",
    "    result = rag.query(q)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"\\nSources:\")\n",
    "    for i, source in enumerate(result['sources'][:2], 1):\n",
    "        print(f\"  {i}. {source}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the RAG Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trace through a single query\n",
    "test_query = \"What frameworks are used for deep learning?\"\n",
    "\n",
    "print(\"Step 1: Retrieve relevant context\")\n",
    "context_docs = rag.retrieve(test_query, n_results=2)\n",
    "print(f\"Found {len(context_docs)} relevant documents:\")\n",
    "for i, doc in enumerate(context_docs, 1):\n",
    "    print(f\"  {i}. {doc}\")\n",
    "\n",
    "print(\"\\nStep 2: Construct prompt with context\")\n",
    "context_text = \"\\n\".join(context_docs)\n",
    "prompt = f\"\"\"Context: {context_text}\n",
    "\n",
    "Question: {test_query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\nStep 3: Generate answer\")\n",
    "answer = rag.generate(test_query, context_docs)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Challenges and Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1: No relevant context\n",
    "print(\"Challenge 1: Question outside knowledge base\")\n",
    "result = rag.query(\"What is quantum mechanics?\")\n",
    "print(f\"Q: {result['question']}\")\n",
    "print(f\"A: {result['answer']}\")\n",
    "print(\"Notice: System should admit it doesn't know\\n\")\n",
    "\n",
    "# Challenge 2: Contradictory information\n",
    "print(\"\\nChallenge 2: Multiple documents\")\n",
    "# Add potentially conflicting info\n",
    "rag.add_documents([\n",
    "    \"Python version 2 was released in 2000.\",\n",
    "    \"Python 3 was released in 2008.\"\n",
    "])\n",
    "result = rag.query(\"When was Python released?\")\n",
    "print(f\"Q: {result['question']}\")\n",
    "print(f\"A: {result['answer']}\")\n",
    "print(\"Notice: May need to clarify which version\\n\")\n",
    "\n",
    "# Challenge 3: Context size limits\n",
    "print(\"\\nChallenge 3: Too much context\")\n",
    "print(\"LLMs have token limits (e.g., 4096, 8192, 128k)\")\n",
    "print(\"Must balance: More context vs token limits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improving RAG Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRAG(SimpleRAG):\n",
    "    \"\"\"RAG with quality improvements.\"\"\"\n",
    "    \n",
    "    def generate(self, query: str, context: list) -> str:\n",
    "        \"\"\"Enhanced generation with better prompting.\"\"\"\n",
    "        context_text = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}]: {doc}\" \n",
    "            for i, doc in enumerate(context)\n",
    "        ])\n",
    "        \n",
    "        # Improved prompt\n",
    "        prompt = f\"\"\"You are a helpful assistant that answers questions based on provided context.\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "1. Answer ONLY using information from the context below\n",
    "2. If the answer is not in the context, say \"I don't have enough information\"\n",
    "3. Cite your sources using [Source N] notation\n",
    "4. Be concise and accurate\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer (with source citations):\"\"\"\n",
    "        \n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,  # Lower temp for factual accuracy\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Test improved RAG\n",
    "improved_rag = ImprovedRAG()\n",
    "improved_rag.add_documents(knowledge)\n",
    "\n",
    "result = improved_rag.query(\"Tell me about Python\")\n",
    "print(\"Improved RAG Response:\")\n",
    "print(result['answer'])\n",
    "print(\"\\nNotice: Should include [Source N] citations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: Enhance the RAG System\n",
    "\n",
    "Improve the SimpleRAG with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "class EnhancedRAG(SimpleRAG):\n",
    "    \"\"\"\n",
    "    Enhanced RAG with:\n",
    "    - Confidence scoring\n",
    "    - Query preprocessing\n",
    "    - Response validation\n",
    "    - Better error handling\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_query(self, query: str) -> str:\n",
    "        \"\"\"Clean and enhance the query.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # - Remove extra whitespace\n",
    "        # - Expand abbreviations\n",
    "        # - Add context if needed\n",
    "        pass\n",
    "    \n",
    "    def assess_confidence(self, query: str, context: list) -> float:\n",
    "        \"\"\"Estimate confidence in the answer.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # - Check context relevance\n",
    "        # - Measure context overlap\n",
    "        # - Return 0-1 confidence score\n",
    "        pass\n",
    "    \n",
    "    def query(self, question: str) -> dict:\n",
    "        \"\"\"Enhanced query with preprocessing and confidence.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # - Preprocess query\n",
    "        # - Retrieve context\n",
    "        # - Assess confidence\n",
    "        # - Generate answer\n",
    "        # - Return result with confidence\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# enhanced = EnhancedRAG()\n",
    "# enhanced.add_documents(knowledge)\n",
    "# result = enhanced.query(\"Your question\")\n",
    "# print(f\"Answer: {result['answer']}\")\n",
    "# print(f\"Confidence: {result['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… What RAG is and why it's useful\n",
    "- âœ… RAG vs fine-tuning tradeoffs\n",
    "- âœ… Basic RAG architecture\n",
    "- âœ… Building a simple RAG system\n",
    "- âœ… Common challenges and solutions\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **RAG = Retrieve + Generate**: Find relevant info, then generate with LLM\n",
    "2. **Better than raw LLM**: Provides current, specific, citable information\n",
    "3. **Easier than fine-tuning**: Quick to build, easy to update\n",
    "4. **Quality matters**: Good retrieval â†’ good answers\n",
    "5. **Prompt engineering**: Critical for RAG quality\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 02: Document Processing Pipeline\n",
    "- ðŸ”— Read about [RAG patterns](https://www.pinecone.io/learn/retrieval-augmented-generation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
