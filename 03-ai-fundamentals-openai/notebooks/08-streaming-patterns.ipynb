{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Streaming Patterns - Advanced\n\n",
        "Production-ready streaming patterns with async, cancellation, and UI enhancements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI, AsyncOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import asyncio\n",
        "import time\n",
        "import tiktoken\n\n",
        "load_dotenv()\n",
        "client = OpenAI()\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 1: Async Streaming\n\n",
        "Stream responses asynchronously for better concurrency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def async_stream_chat(message: str) -> str:\n",
        "    \"\"\"\n",
        "    Stream chat response asynchronously.\n",
        "    \"\"\"\n",
        "    stream = await async_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    \n",
        "    async for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        if delta:\n",
        "            full_response += delta\n",
        "            print(delta, end='', flush=True)\n",
        "    \n",
        "    print()  # Newline\n",
        "    return full_response\n\n",
        "# Test async streaming\n",
        "response = await async_stream_chat(\"Tell me a fun fact about space.\")\n",
        "print(f\"\\nâœ… Completed: {len(response)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 2: Multiple Concurrent Streams\n\n",
        "Stream multiple responses at the same time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def labeled_stream(label: str, message: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Stream with label prefix for concurrent streams.\n",
        "    \"\"\"\n",
        "    stream = await async_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        stream=True,\n",
        "        max_tokens=50  # Keep short for demo\n",
        "    )\n",
        "    \n",
        "    full_response = \"\"\n",
        "    \n",
        "    async for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        if delta:\n",
        "            full_response += delta\n",
        "            print(f\"[{label}] {delta}\", end='', flush=True)\n",
        "    \n",
        "    print(f\"\\n[{label}] âœ“ Done\")\n",
        "    return label, full_response\n\n",
        "# Stream 3 responses concurrently\n",
        "async def concurrent_streams():\n",
        "    tasks = [\n",
        "        labeled_stream(\"A\", \"Fun fact about Python?\"),\n",
        "        labeled_stream(\"B\", \"Fun fact about JavaScript?\"),\n",
        "        labeled_stream(\"C\", \"Fun fact about Rust?\")\n",
        "    ]\n",
        "    \n",
        "    results = await asyncio.gather(*tasks)\n",
        "    return results\n\n",
        "print(\"Streaming 3 responses concurrently:\\n\")\n",
        "results = await concurrent_streams()\n",
        "print(f\"\\nâœ… All {len(results)} streams complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 3: Progress Indicators\n\n",
        "Show progress while streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n\n",
        "def stream_with_progress(message: str):\n",
        "    \"\"\"\n",
        "    Stream with word count progress indicator.\n",
        "    \"\"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_content = \"\"\n",
        "    word_count = 0\n",
        "    \n",
        "    print(\"Response: \", end='', flush=True)\n",
        "    \n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        \n",
        "        if delta:\n",
        "            full_content += delta\n",
        "            \n",
        "            # Count words in delta\n",
        "            word_count += len(delta.split())\n",
        "            \n",
        "            # Display content\n",
        "            print(delta, end='', flush=True)\n",
        "    \n",
        "    print(f\"\\n\\nðŸ“Š Progress: {word_count} words, {len(full_content)} characters\")\n",
        "    return full_content\n\n",
        "result = stream_with_progress(\"Write a paragraph about AI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 4: Real-Time Token Counter\n\n",
        "Count tokens as they stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_with_live_token_count(message: str, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Stream with real-time token counting.\n",
        "    \"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    \n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_content = \"\"\n",
        "    token_count = 0\n",
        "    update_interval = 5  # Update count every N tokens\n",
        "    \n",
        "    print(\"Streaming... \", end='', flush=True)\n",
        "    \n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        \n",
        "        if delta:\n",
        "            full_content += delta\n",
        "            print(delta, end='', flush=True)\n",
        "            \n",
        "            # Update token count periodically\n",
        "            new_token_count = len(encoding.encode(full_content))\n",
        "            if new_token_count >= token_count + update_interval:\n",
        "                token_count = new_token_count\n",
        "                # Print token count inline\n",
        "                print(f\" [{token_count}t]\", end='', flush=True)\n",
        "    \n",
        "    final_tokens = len(encoding.encode(full_content))\n",
        "    print(f\"\\n\\nðŸ“Š Final token count: {final_tokens}\")\n",
        "    \n",
        "    return full_content, final_tokens\n\n",
        "content, tokens = stream_with_live_token_count(\n",
        "    \"Explain how neural networks learn.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 5: Cancellation Support\n\n",
        "Allow users to stop streaming mid-response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import threading\n\n",
        "class CancellableStream:\n",
        "    \"\"\"\n",
        "    Stream that can be cancelled.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.cancelled = False\n",
        "    \n",
        "    def cancel(self):\n",
        "        \"\"\"Cancel the stream.\"\"\"\n",
        "        self.cancelled = True\n",
        "        print(\"\\n\\nâš ï¸  Stream cancelled by user!\")\n",
        "    \n",
        "    def stream(self, message: str, model=\"gpt-3.5-turbo\"):\n",
        "        \"\"\"Stream with cancellation support.\"\"\"\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": message}],\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        full_content = \"\"\n",
        "        \n",
        "        print(\"Streaming (will auto-cancel after 3 seconds for demo)...\\n\")\n",
        "        \n",
        "        for chunk in stream:\n",
        "            if self.cancelled:\n",
        "                break\n",
        "            \n",
        "            delta = chunk.choices[0].delta.content\n",
        "            if delta:\n",
        "                full_content += delta\n",
        "                print(delta, end='', flush=True)\n",
        "        \n",
        "        if not self.cancelled:\n",
        "            print(\"\\n\\nâœ… Stream completed naturally\")\n",
        "        \n",
        "        return full_content\n\n",
        "# Demo: Auto-cancel after 3 seconds\n",
        "cancellable = CancellableStream()\n\n",
        "# Set up auto-cancel for demo\n",
        "timer = threading.Timer(3.0, cancellable.cancel)\n",
        "timer.start()\n\n",
        "result = cancellable.stream(\"Write a long essay about space exploration.\")\n",
        "timer.cancel()  # Clean up timer\n\n",
        "print(f\"\\nReceived {len(result)} characters before cancellation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 6: Buffered Streaming\n\n",
        "Buffer chunks for smoother display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n\n",
        "def buffered_stream(message: str, buffer_size: int = 5):\n",
        "    \"\"\"\n",
        "    Stream with buffering for smoother display.\n",
        "    \n",
        "    Args:\n",
        "        message: User message\n",
        "        buffer_size: Number of tokens to buffer before displaying\n",
        "    \"\"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}],\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    buffer = \"\"\n",
        "    full_content = \"\"\n",
        "    \n",
        "    print(\"Buffered streaming:\\n\")\n",
        "    \n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        \n",
        "        if delta:\n",
        "            buffer += delta\n",
        "            \n",
        "            # Flush buffer when it reaches size\n",
        "            if len(buffer.split()) >= buffer_size:\n",
        "                print(buffer, end='', flush=True)\n",
        "                full_content += buffer\n",
        "                buffer = \"\"\n",
        "                time.sleep(0.1)  # Simulate smooth display\n",
        "    \n",
        "    # Flush remaining buffer\n",
        "    if buffer:\n",
        "        print(buffer, end='', flush=True)\n",
        "        full_content += buffer\n",
        "    \n",
        "    print(\"\\n\\n(Notice smoother 'chunked' display)\")\n",
        "    return full_content\n\n",
        "result = buffered_stream(\"Describe machine learning in simple terms.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pattern 7: Conversation Streaming\n\n",
        "Stream within a conversation manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StreamingConversation:\n",
        "    \"\"\"\n",
        "    Conversation manager with streaming support.\n",
        "    \"\"\"\n",
        "    def __init__(self, system_prompt: str = \"You are a helpful assistant.\"):\n",
        "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        self.client = OpenAI()\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    \n",
        "    def stream_reply(self, user_message: str) -> dict:\n",
        "        \"\"\"Send message and stream response.\"\"\"\n",
        "        # Add user message\n",
        "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        \n",
        "        # Stream response\n",
        "        stream = self.client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=self.messages,\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        full_response = \"\"\n",
        "        \n",
        "        print(\"Assistant: \", end='', flush=True)\n",
        "        \n",
        "        for chunk in stream:\n",
        "            delta = chunk.choices[0].delta.content\n",
        "            if delta:\n",
        "                full_response += delta\n",
        "                print(delta, end='', flush=True)\n",
        "        \n",
        "        print()  # Newline\n",
        "        \n",
        "        # Add assistant response to history\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
        "        \n",
        "        return {\n",
        "            'content': full_response,\n",
        "            'tokens': len(self.encoding.encode(full_response)),\n",
        "            'turn': len([m for m in self.messages if m['role'] == 'user'])\n",
        "        }\n\n",
        "# Test conversation streaming\n",
        "conv = StreamingConversation()\n\n",
        "print(\"User: What is Python?\")\n",
        "result1 = conv.stream_reply(\"What is Python?\")\n",
        "\n",
        "print(f\"\\n[{result1['tokens']} tokens]\\n\")\n",
        "\n",
        "print(\"User: What can I build with it?\")\n",
        "result2 = conv.stream_reply(\"What can I build with it?\")\n",
        "\n",
        "print(f\"\\n[{result2['tokens']} tokens]\")\n",
        "print(f\"\\nâœ… Conversation has {result2['turn']} turns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Tips\n\n",
        "### Performance\n",
        "- Use async streaming for handling multiple users\n",
        "- Buffer tokens for smoother UI updates\n",
        "- Don't update UI on every single token (rate limit updates)\n\n",
        "### UX\n",
        "- Always show streaming progress\n",
        "- Provide cancel button for long responses\n",
        "- Show \"typing\" indicator before first token\n",
        "- Display token/cost estimates\n\n",
        "### Error Handling\n",
        "- Stream can fail mid-response\n",
        "- Always have fallback for network errors\n",
        "- Save partial responses on error\n",
        "- Implement retry logic\n\n",
        "### Token Management\n",
        "- Count tokens manually (not in stream)\n",
        "- Set max_tokens to prevent runaway costs\n",
        "- Track streaming costs same as non-streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercises\n\n",
        "1. Build async streaming chat that handles 10 concurrent users\n",
        "2. Add progress bar that fills as response streams\n",
        "3. Implement \"Stop\" button that cancels stream\n",
        "4. Create buffered display with typing animation\n",
        "5. Build streaming conversation with save/load support"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
