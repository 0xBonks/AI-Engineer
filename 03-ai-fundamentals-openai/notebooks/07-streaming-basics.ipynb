{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Streaming Responses - Basics\n\n",
        "## Why Streaming?\n\n",
        "**Problem:** Traditional API calls wait for the entire response before showing anything.\n",
        "- GPT-4 can take 30-60 seconds to generate a response\n",
        "- User sees nothing until complete\n",
        "- Poor user experience\n\n",
        "**Solution:** Streaming displays tokens as they're generated!\n",
        "- Response appears immediately\n",
        "- Shows progress in real-time  \n",
        "- Better perceived performance\n",
        "- Like ChatGPT's interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import time\n\n",
        "load_dotenv()\n",
        "client = OpenAI()\n\n",
        "print(\"‚úÖ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Streaming\n\n",
        "Set `stream=True` to enable streaming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Non-streaming (traditional)\n",
        "print(\"Non-streaming (wait for complete response):\")\n",
        "start = time.time()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 10 slowly.\"}],\n",
        "    stream=False\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"Time: {time.time() - start:.2f}s\\n\")\n\n",
        "# Streaming\n",
        "print(\"Streaming (see tokens as generated):\")\n",
        "start = time.time()\n\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Count from 1 to 10 slowly.\"}],\n",
        "    stream=True  # Enable streaming!\n",
        ")\n\n",
        "for chunk in stream:\n",
        "    # Extract content from chunk\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
        "\n",
        "print(f\"\\nTime: {time.time() - start:.2f}s\")\n",
        "print(\"\\n(Notice: Same total time, but you see progress immediately!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stream Chunk Structure\n\n",
        "Each chunk contains incremental data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Inspecting stream chunks:\\n\")\n\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say 'Hello World'\"}],\n",
        "    stream=True\n",
        ")\n\n",
        "for i, chunk in enumerate(stream):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    print(f\"  ID: {chunk.id}\")\n",
        "    print(f\"  Delta: {chunk.choices[0].delta}\")\n",
        "    print(f\"  Finish Reason: {chunk.choices[0].finish_reason}\")\n",
        "    print()\n",
        "    \n",
        "    if i >= 5:  # Limit output for demo\n",
        "        print(\"... (remaining chunks omitted)\")\n",
        "        # Consume rest of stream\n",
        "        for _ in stream:\n",
        "            pass\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Complete Response\n\n",
        "Collect chunks to build the full message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stream_completion(messages, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Stream a completion and return the full response.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (full_content, chunk_count)\n",
        "    \"\"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_content = \"\"\n",
        "    chunk_count = 0\n",
        "    \n",
        "    for chunk in stream:\n",
        "        chunk_count += 1\n",
        "        \n",
        "        # Extract delta content\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        \n",
        "        if delta is not None:\n",
        "            full_content += delta\n",
        "            print(delta, end='', flush=True)\n",
        "    \n",
        "    print()  # Newline at end\n",
        "    return full_content, chunk_count\n\n",
        "# Test it\n",
        "messages = [{\"role\": \"user\", \"content\": \"Write a haiku about coding.\"}]\n",
        "content, chunks = stream_completion(messages)\n",
        "\n",
        "print(f\"\\nüìä Received {chunks} chunks\")\n",
        "print(f\"üìù Total content length: {len(content)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token Counting with Streaming\n\n",
        "**Challenge:** Streaming doesn't include token counts in chunks!\n",
        "\n",
        "**Solutions:**\n",
        "1. Count tokens manually with tiktoken\n",
        "2. Make a final non-streaming call with max_tokens=1 to get usage (hack)\n",
        "3. Estimate based on content length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n\n",
        "def stream_with_token_count(messages, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Stream response and count tokens manually.\n",
        "    \"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    \n",
        "    # Count input tokens\n",
        "    input_tokens = sum(\n",
        "        len(encoding.encode(msg['content'])) \n",
        "        for msg in messages\n",
        "    )\n",
        "    \n",
        "    # Stream response\n",
        "    stream = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    full_content = \"\"\n",
        "    \n",
        "    for chunk in stream:\n",
        "        delta = chunk.choices[0].delta.content\n",
        "        if delta:\n",
        "            full_content += delta\n",
        "            print(delta, end='', flush=True)\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # Count output tokens\n",
        "    output_tokens = len(encoding.encode(full_content))\n",
        "    total_tokens = input_tokens + output_tokens\n",
        "    \n",
        "    return {\n",
        "        'content': full_content,\n",
        "        'prompt_tokens': input_tokens,\n",
        "        'completion_tokens': output_tokens,\n",
        "        'total_tokens': total_tokens\n",
        "    }\n\n",
        "# Test it\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain recursion briefly.\"}]\n",
        "result = stream_with_token_count(messages)\n",
        "\n",
        "print(f\"\\nüìä Token Usage:\")\n",
        "print(f\"  Input: {result['prompt_tokens']}\")\n",
        "print(f\"  Output: {result['completion_tokens']}\")\n",
        "print(f\"  Total: {result['total_tokens']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling in Streams\n\n",
        "Streams can fail mid-generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_stream(messages, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Stream with proper error handling.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        stream = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            stream=True\n",
        "        )\n",
        "        \n",
        "        full_content = \"\"\n",
        "        \n",
        "        for chunk in stream:\n",
        "            delta = chunk.choices[0].delta.content\n",
        "            \n",
        "            if delta:\n",
        "                full_content += delta\n",
        "                print(delta, end='', flush=True)\n",
        "            \n",
        "            # Check for finish\n",
        "            finish_reason = chunk.choices[0].finish_reason\n",
        "            if finish_reason:\n",
        "                print(f\"\\n\\n[Finished: {finish_reason}]\")\n",
        "        \n",
        "        return full_content\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n\\n‚ùå Stream error: {e}\")\n",
        "        return None\n\n",
        "# Test it\n",
        "messages = [{\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n",
        "result = safe_stream(messages)\n",
        "\n",
        "if result:\n",
        "    print(f\"\\n‚úÖ Successfully received {len(result)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming with Temperature\n\n",
        "Higher temperature = more varied token selection = interesting streaming effect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Tell a very short story.\"}]\n\n",
        "print(\"Temperature 0 (deterministic):\")\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    stream=True\n",
        ")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
        "\n",
        "print(\"\\n\\nTemperature 1.8 (creative):\")\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature=1.8,\n",
        "    stream=True\n",
        ")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
        "\n",
        "print(\"\\n\\n(Notice how temperature affects what you see being generated!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Practices\n\n",
        "### ‚úÖ DO:\n",
        "- Use streaming for user-facing applications\n",
        "- Display tokens immediately for better UX\n",
        "- Handle errors gracefully (stream can fail mid-response)\n",
        "- Count tokens manually with tiktoken\n",
        "- Show finish_reason to user if needed\n\n",
        "### ‚ùå DON'T:\n",
        "- Use streaming if you need the full response before processing\n",
        "- Rely on API for token counts (not in stream chunks)\n",
        "- Forget to flush output buffers (use flush=True)\n",
        "- Ignore finish_reason (could be 'length' or 'content_filter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercises\n\n",
        "1. Build a streaming chat that shows each word as it appears\n",
        "2. Add a \"Stop\" button to cancel streaming mid-response\n",
        "3. Display real-time token count while streaming\n",
        "4. Create a \"typing indicator\" animation before first token\n",
        "5. Build a comparison tool that shows streaming vs non-streaming side-by-side"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
