{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# First OpenAI API Call\n\n",
        "Let's make your first call to the OpenAI Chat Completions API!\n\n",
        "## Setup\n\n",
        "Make sure you have:\n",
        "1. OpenAI Python library installed: `pip install openai`\n",
        "2. API key in `.env` file: `OPENAI_API_KEY=sk-...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n\n",
        "# Load environment variables\n",
        "load_dotenv()\n\n",
        "# Create client\n",
        "client = OpenAI()  # Uses OPENAI_API_KEY from environment\n\n",
        "print(\"âœ… OpenAI client initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Simple Question\n\n",
        "Let's ask a simple factual question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make API call\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ]\n",
        ")\n\n",
        "# Extract response\n",
        "answer = response.choices[0].message.content\n",
        "print(\"Answer:\", answer)\n",
        "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: With System Prompt\n\n",
        "System prompts guide the assistant's behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that explains concepts to a 5-year-old.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is gravity?\"}\n",
        "    ]\n",
        ")\n\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Temperature Control\n\n",
        "Temperature affects randomness. Let's see the difference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Write a creative tagline for a coffee shop.\"\n\n",
        "# Low temperature (deterministic)\n",
        "response_low = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0.2\n",
        ")\n\n",
        "# High temperature (creative)\n",
        "response_high = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=1.5\n",
        ")\n\n",
        "print(\"Temperature 0.2 (focused):\")\n",
        "print(response_low.choices[0].message.content)\n",
        "print(\"\\nTemperature 1.5 (creative):\")\n",
        "print(response_high.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: max_tokens Control\n\n",
        "Limit response length with max_tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n",
        "    ],\n",
        "    max_tokens=30  # Limit to ~20-25 words\n",
        ")\n\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"\\nCompletion tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"Finish reason: {response.choices[0].finish_reason}\")  # May be 'length' if cut off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling\n\n",
        "Always handle potential errors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAIError, RateLimitError, APIError\n\n",
        "def safe_api_call(messages, model=\"gpt-3.5-turbo\", **kwargs):\n",
        "    \"\"\"Make API call with error handling.\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            **kwargs\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    \n",
        "    except RateLimitError:\n",
        "        return \"Error: Rate limit exceeded. Please wait and try again.\"\n",
        "    \n",
        "    except APIError as e:\n",
        "        return f\"API Error: {e}\"\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Unexpected error: {e}\"\n\n",
        "# Test it\n",
        "result = safe_api_call([\n",
        "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
        "])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parsing Response Structure\n\n",
        "Understanding what you get back from the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Say 'hi'\"}]\n",
        ")\n\n",
        "print(\"Full response structure:\")\n",
        "print(f\"ID: {response.id}\")\n",
        "print(f\"Model: {response.model}\")\n",
        "print(f\"Created: {response.created}\")\n",
        "print(f\"\\nMessage content: {response.choices[0].message.content}\")\n",
        "print(f\"Finish reason: {response.choices[0].finish_reason}\")\n",
        "print(f\"\\nUsage:\")\n",
        "print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"  Total tokens: {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Async API Calls\n\n",
        "For better performance when making multiple requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from openai import AsyncOpenAI\n\n",
        "async_client = AsyncOpenAI()\n\n",
        "async def async_chat(message: str) -> str:\n",
        "    \"\"\"Make async API call.\"\"\"\n",
        "    response = await async_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n\n",
        "# Make multiple calls concurrently\n",
        "async def main():\n",
        "    questions = [\n",
        "        \"What is Python?\",\n",
        "        \"What is JavaScript?\",\n",
        "        \"What is TypeScript?\"\n",
        "    ]\n",
        "    \n",
        "    # Run concurrently!\n",
        "    results = await asyncio.gather(*[async_chat(q) for q in questions])\n",
        "    \n",
        "    for question, answer in zip(questions, results):\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {answer}\")\n",
        "        print()\n\n",
        "# Run async function\n",
        "await main()  # In Jupyter\n",
        "# asyncio.run(main())  # In regular Python script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cost Tracking\n\n",
        "Track how much each request costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cost(usage, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Calculate cost in USD.\"\"\"\n",
        "    # Pricing (as of 2024)\n",
        "    pricing = {\n",
        "        \"gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},  # per 1k tokens\n",
        "        \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
        "        \"gpt-4\": {\"input\": 0.03, \"output\": 0.06}\n",
        "    }\n",
        "    \n",
        "    rates = pricing.get(model, pricing[\"gpt-3.5-turbo\"])\n",
        "    \n",
        "    input_cost = (usage.prompt_tokens / 1000) * rates[\"input\"]\n",
        "    output_cost = (usage.completion_tokens / 1000) * rates[\"output\"]\n",
        "    \n",
        "    return input_cost + output_cost\n\n",
        "# Make a call and track cost\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Explain photosynthesis.\"}]\n",
        ")\n\n",
        "cost = calculate_cost(response.usage)\n",
        "print(f\"Tokens used: {response.usage.total_tokens}\")\n",
        "print(f\"Cost: ${cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice Exercise\n\n",
        "Try these on your own:\n\n",
        "1. Make a call with temperature=0 and run it 3 times. Are results identical?\n",
        "2. Make a call with temperature=1.8 and run it 3 times. How different are results?\n",
        "3. Create a system prompt that makes the assistant respond as a medieval knight\n",
        "4. Calculate the cost of 1000 API calls with average 100 input / 200 output tokens\n",
        "5. Write an async function that calls the API 10 times concurrently"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
