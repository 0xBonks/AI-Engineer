{
  "cells": [
    {"cell_type": "markdown", "metadata": {}, "source": ["# Few-Shot Learning\n\n", "Teach the model through examples!\n\n", "## What is Few-Shot Learning?\n\n", "- **0-shot**: No examples (just ask)\n", "- **1-shot**: One example\n", "- **Few-shot**: Multiple examples (typically 2-5)\n\n", "The model learns the pattern from your examples."]},
    {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from openai import OpenAI\n", "from dotenv import load_dotenv\n\n", "load_dotenv()\n", "client = OpenAI()"]},
    {"cell_type": "markdown", "metadata": {}, "source": ["## Example 1: 0-Shot vs Few-Shot"]},
    {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 0-shot: No examples\n", "response_0 = client.chat.completions.create(\n", "    model=\"gpt-3.5-turbo\",\n", "    messages=[{\"role\": \"user\", \"content\": \"Classify sentiment: This movie was amazing!\"}]\n", ")\n", "print(\"0-SHOT:\")\n", "print(response_0.choices[0].message.content)\n\n", "# Few-shot: With examples\n", "response_few = client.chat.completions.create(\n", "    model=\"gpt-3.5-turbo\",\n", "    messages=[\n", "        {\"role\": \"user\", \"content\": \"Classify sentiment: I loved this product!\"},\n", "        {\"role\": \"assistant\", \"content\": \"Positive\"},\n", "        {\"role\": \"user\", \"content\": \"Classify sentiment: Terrible experience.\"},\n", "        {\"role\": \"assistant\", \"content\": \"Negative\"},\n", "        {\"role\": \"user\", \"content\": \"Classify sentiment: This movie was amazing!\"}\n", "    ]\n", ")\n", "print(\"\\nFEW-SHOT:\")\n", "print(response_few.choices[0].message.content)"]},
    {"cell_type": "markdown", "metadata": {}, "source": ["## Example 2: Format Learning\n\n", "Teach the model your desired output format."]},
    {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["messages = [\n", "    # Example 1\n", "    {\"role\": \"user\", \"content\": \"Extract info: John Smith, age 30, lives in NYC\"},\n", "    {\"role\": \"assistant\", \"content\": '{\"name\": \"John Smith\", \"age\": 30, \"city\": \"NYC\"}'},\n", "    # Example 2\n", "    {\"role\": \"user\", \"content\": \"Extract info: Sarah Lee, age 25, lives in SF\"},\n", "    {\"role\": \"assistant\", \"content\": '{\"name\": \"Sarah Lee\", \"age\": 25, \"city\": \"SF\"}'},\n", "    # Actual task\n", "    {\"role\": \"user\", \"content\": \"Extract info: Bob Johnson, age 45, lives in Seattle\"}\n", "]\n\n", "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=messages)\n", "print(response.choices[0].message.content)"]},
    {"cell_type": "markdown", "metadata": {}, "source": ["## Best Practices\n\n", "1. **2-5 examples** is usually optimal\n", "2. **Diverse examples** cover edge cases\n", "3. **Consistent format** in all examples\n", "4. **Quality > Quantity** - good examples matter more\n", "5. **Order matters** - most relevant examples first"]}
  ],
  "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}},
  "nbformat": 4,
  "nbformat_minor": 4
}
