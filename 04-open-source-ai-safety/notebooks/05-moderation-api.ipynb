{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04 - Notebook 05: Content Moderation API\n",
    "\n",
    "## Learning Objectives\n",
    "- Use OpenAI Moderation API\n",
    "- Filter harmful content automatically\n",
    "- Implement custom moderation logic\n",
    "- Handle edge cases and false positives\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenAI Moderation API Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moderate_content(text: str) -> dict:\n",
    "    \"\"\"Check content with OpenAI Moderation API.\"\"\"\n",
    "    \n",
    "    response = client.moderations.create(input=text)\n",
    "    result = response.results[0]\n",
    "    \n",
    "    return {\n",
    "        \"flagged\": result.flagged,\n",
    "        \"categories\": result.categories.model_dump(),\n",
    "        \"category_scores\": result.category_scores.model_dump()\n",
    "    }\n",
    "\n",
    "# Test with safe content\n",
    "safe_text = \"I love learning about artificial intelligence!\"\n",
    "result = moderate_content(safe_text)\n",
    "\n",
    "print(f\"Text: {safe_text}\")\n",
    "print(f\"Flagged: {result['flagged']}\")\n",
    "print(f\"\\nCategories flagged: {[k for k, v in result['categories'].items() if v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Moderation Categories\n",
    "\n",
    "The API checks for:\n",
    "- **hate**: Hateful content\n",
    "- **hate/threatening**: Hateful with violence\n",
    "- **harassment**: Harassment\n",
    "- **harassment/threatening**: Harassment with violence\n",
    "- **self-harm**: Self-harm content\n",
    "- **self-harm/intent**: Intent to self-harm\n",
    "- **self-harm/instructions**: Instructions for self-harm\n",
    "- **sexual**: Sexual content\n",
    "- **sexual/minors**: Sexual content involving minors\n",
    "- **violence**: Violent content\n",
    "- **violence/graphic**: Graphic violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various content types\n",
    "test_texts = [\n",
    "    \"This is a normal, friendly message.\",\n",
    "    \"I want to hurt myself.\",  # Self-harm\n",
    "    \"I hate everyone and want to cause violence.\",  # Hate + Violence\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = moderate_content(text)\n",
    "    status = \"ðŸš¨ FLAGGED\" if result[\"flagged\"] else \"âœ“ Safe\"\n",
    "    \n",
    "    print(f\"{status}: {text[:50]}\")\n",
    "    if result[\"flagged\"]:\n",
    "        flagged_cats = [k for k, v in result[\"categories\"].items() if v]\n",
    "        print(f\"  Categories: {', '.join(flagged_cats)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Moderated Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeratedChatbot:\n",
    "    \"\"\"Chatbot with input/output moderation.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI):\n",
    "        self.client = client\n",
    "    \n",
    "    def is_safe(self, text: str) -> tuple[bool, dict]:\n",
    "        \"\"\"Check if text passes moderation.\"\"\"\n",
    "        result = moderate_content(text)\n",
    "        return not result[\"flagged\"], result\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"Chat with input/output moderation.\"\"\"\n",
    "        \n",
    "        # Moderate input\n",
    "        input_safe, input_result = self.is_safe(user_input)\n",
    "        if not input_safe:\n",
    "            flagged = [k for k, v in input_result[\"categories\"].items() if v]\n",
    "            return f\"âš ï¸ Your message was flagged for: {', '.join(flagged)}. Please rephrase.\"\n",
    "        \n",
    "        # Get response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful, respectful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        output = response.choices[0].message.content\n",
    "        \n",
    "        # Moderate output\n",
    "        output_safe, output_result = self.is_safe(output)\n",
    "        if not output_safe:\n",
    "            return \"âš ï¸ I cannot provide that response. Let me try again.\"\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test moderated chatbot\n",
    "bot = ModeratedChatbot(client)\n",
    "\n",
    "test_inputs = [\n",
    "    \"What's the weather like?\",\n",
    "    \"I hate you and want to hurt you\",  # Will be blocked\n",
    "    \"Tell me about machine learning\"\n",
    "]\n",
    "\n",
    "for inp in test_inputs:\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"Bot: {bot.chat(inp)}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Score-Based Moderation\n",
    "\n",
    "Use scores for fine-grained control instead of binary flagged/not flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detailed_moderation(text: str) -> pd.DataFrame:\n",
    "    \"\"\"Show detailed category scores.\"\"\"\n",
    "    \n",
    "    result = moderate_content(text)\n",
    "    scores = result[\"category_scores\"]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {\"Category\": k, \"Score\": f\"{v:.4f}\", \"Flagged\": result[\"categories\"][k]}\n",
    "        for k, v in scores.items()\n",
    "    ])\n",
    "    \n",
    "    return df.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "# Analyze borderline content\n",
    "text = \"I'm really frustrated with this situation.\"\n",
    "print(f\"Analyzing: {text}\\n\")\n",
    "print(detailed_moderation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModeration:\n",
    "    \"\"\"Moderation with custom thresholds.\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds: dict = None):\n",
    "        # Default: More strict for violence, less strict for others\n",
    "        self.thresholds = thresholds or {\n",
    "            \"violence\": 0.3,  # Lower = more strict\n",
    "            \"hate\": 0.5,\n",
    "            \"harassment\": 0.5,\n",
    "            \"self-harm\": 0.4,\n",
    "            \"sexual\": 0.6,\n",
    "        }\n",
    "    \n",
    "    def moderate(self, text: str) -> dict:\n",
    "        \"\"\"Apply custom thresholds.\"\"\"\n",
    "        result = moderate_content(text)\n",
    "        scores = result[\"category_scores\"]\n",
    "        \n",
    "        violations = []\n",
    "        for category, threshold in self.thresholds.items():\n",
    "            # Check all related categories\n",
    "            for cat_name, score in scores.items():\n",
    "                if category in cat_name and score > threshold:\n",
    "                    violations.append((cat_name, score))\n",
    "        \n",
    "        return {\n",
    "            \"blocked\": len(violations) > 0,\n",
    "            \"violations\": violations,\n",
    "            \"original_flagged\": result[\"flagged\"]\n",
    "        }\n",
    "\n",
    "# Test custom moderation\n",
    "moderator = CustomModeration()\n",
    "\n",
    "test = \"I'm going to destroy my computer!\"\n",
    "result = moderator.moderate(test)\n",
    "\n",
    "print(f\"Text: {test}\")\n",
    "print(f\"Custom blocked: {result['blocked']}\")\n",
    "print(f\"OpenAI flagged: {result['original_flagged']}\")\n",
    "print(f\"Violations: {result['violations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_moderate(texts: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Moderate multiple texts efficiently.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        mod_result = moderate_content(text)\n",
    "        results.append({\n",
    "            \"text\": text[:50] + \"...\" if len(text) > 50 else text,\n",
    "            \"flagged\": mod_result[\"flagged\"],\n",
    "            \"categories\": \", \".join([k for k, v in mod_result[\"categories\"].items() if v]) or \"None\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test batch moderation\n",
    "comments = [\n",
    "    \"This is a great product!\",\n",
    "    \"I hate this so much\",\n",
    "    \"The quality is terrible\",\n",
    "    \"Amazing experience, highly recommend\",\n",
    "    \"You're all idiots\"\n",
    "]\n",
    "\n",
    "print(\"Batch Moderation Results:\\n\")\n",
    "print(batch_moderate(comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_moderation(text: str, context: str = None) -> dict:\n",
    "    \"\"\"Moderation with context awareness.\"\"\"\n",
    "    \n",
    "    result = moderate_content(text)\n",
    "    \n",
    "    # Add context-based overrides\n",
    "    if context == \"medical\" and result[\"categories\"].get(\"self-harm\", False):\n",
    "        # Medical discussions might trigger false positives\n",
    "        if \"treatment\" in text.lower() or \"therapy\" in text.lower():\n",
    "            result[\"flagged\"] = False\n",
    "            result[\"override_reason\"] = \"Medical context\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test context-aware moderation\n",
    "medical_text = \"I'm learning about self-harm prevention and treatment options.\"\n",
    "\n",
    "print(\"Without context:\")\n",
    "print(moderate_content(medical_text)[\"flagged\"])\n",
    "\n",
    "print(\"\\nWith medical context:\")\n",
    "print(smart_moderation(medical_text, context=\"medical\")[\"flagged\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Comment Moderation System\n",
    "\n",
    "Create a system that:\n",
    "1. Accepts user comments\n",
    "2. Moderates using OpenAI API\n",
    "3. Logs flagged content\n",
    "4. Provides user-friendly feedback\n",
    "5. Allows appeals for false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "class CommentModerationSystem:\n",
    "    \"\"\"Full-featured comment moderation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.flagged_log = []\n",
    "    \n",
    "    def submit_comment(self, user_id: str, comment: str) -> dict:\n",
    "        \"\"\"Submit and moderate a comment.\"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def appeal(self, comment_id: str, reason: str) -> bool:\n",
    "        \"\"\"Appeal a moderation decision.\"\"\"\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "# Test your system\n",
    "# system = CommentModerationSystem()\n",
    "# system.submit_comment(\"user123\", \"This is inappropriate content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Using OpenAI Moderation API\n",
    "- âœ… Understanding moderation categories\n",
    "- âœ… Building moderated chatbots\n",
    "- âœ… Custom thresholds and batch processing\n",
    "- âœ… Handling false positives\n",
    "\n",
    "## Best Practices\n",
    "1. **Moderate both input and output**\n",
    "2. **Use scores for nuanced decisions**\n",
    "3. **Provide clear user feedback**\n",
    "4. **Log flagged content for review**\n",
    "5. **Allow appeals for false positives**\n",
    "6. **Consider context when possible**\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 06: Bias Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
