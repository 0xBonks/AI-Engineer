{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04 - Notebook 02: Hugging Face Inference SDK\n",
    "\n",
    "## Learning Objectives\n",
    "- Use Hugging Face Inference API for model inference\n",
    "- Compare inference performance across models\n",
    "- Handle rate limits and errors\n",
    "- Understand pricing and quotas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface-hub python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Initialize inference client\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "print(\"âœ“ Inference client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation\n",
    "prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "response = client.text_generation(\n",
    "    prompt,\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat-based interaction\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to check if a number is prime.\"}\n",
    "]\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=messages,\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"Assistant:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the response token by token\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for token in client.text_generation(\n",
    "    prompt,\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    max_new_tokens=200,\n",
    "    stream=True\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print(\"\\n\\nâœ“ Streaming complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models on the same prompt\n",
    "prompt = \"What are the three laws of robotics?\"\n",
    "\n",
    "models = [\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        response = client.text_generation(\n",
    "            prompt,\n",
    "            model=model,\n",
    "            max_new_tokens=150\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(response)\n",
    "        print(f\"\\nâ±ï¸ Time: {elapsed:.2f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Rate Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfHubHTTPError\n",
    "import time\n",
    "\n",
    "def safe_inference(prompt: str, model: str, max_retries: int = 3):\n",
    "    \"\"\"Inference with retry logic for rate limits.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.text_generation(\n",
    "                prompt,\n",
    "                model=model,\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "            return response\n",
    "            \n",
    "        except HfHubHTTPError as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"âš ï¸ Rate limited. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "# Test the function\n",
    "result = safe_inference(\n",
    "    \"What is machine learning?\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task-Specific APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, \n",
    "in contrast to the natural intelligence displayed by humans and animals. \n",
    "Leading AI textbooks define the field as the study of \"intelligent agents\": \n",
    "any device that perceives its environment and takes actions that maximize \n",
    "its chance of successfully achieving its goals. Colloquially, the term \n",
    "\"artificial intelligence\" is often used to describe machines that mimic \n",
    "\"cognitive\" functions that humans associate with the human mind, such as \n",
    "\"learning\" and \"problem solving\".\n",
    "\"\"\"\n",
    "\n",
    "summary = client.summarization(long_text)\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Model Testing Harness\n",
    "\n",
    "Create a function that:\n",
    "1. Tests multiple models with the same prompt\n",
    "2. Measures response time for each\n",
    "3. Compares output quality\n",
    "4. Handles errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "def test_models(prompt: str, models: list) -> dict:\n",
    "    \"\"\"Test multiple models and return comparison.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test your function\n",
    "# results = test_models(\n",
    "#     \"Explain neural networks\",\n",
    "#     [\"mistralai/Mistral-7B-Instruct-v0.3\", \"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Basic inference with Hugging Face API\n",
    "- âœ… Chat completions and streaming\n",
    "- âœ… Comparing models\n",
    "- âœ… Error handling and rate limits\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 03: Ollama Local Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
