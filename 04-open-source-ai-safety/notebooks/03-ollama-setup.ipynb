{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04 - Notebook 03: Ollama Local Setup\n",
    "\n",
    "## Learning Objectives\n",
    "- Install and configure Ollama\n",
    "- Pull and manage local models\n",
    "- Run inference locally with Python SDK\n",
    "- Compare local vs cloud inference\n",
    "\n",
    "## Prerequisites\n",
    "- 8GB+ RAM recommended\n",
    "- Ollama installed (https://ollama.ai)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Ollama\n",
    "\n",
    "```bash\n",
    "# macOS/Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Windows\n",
    "# Download from https://ollama.com/download/windows\n",
    "\n",
    "# Verify installation\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pulling Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a model (run in terminal first)\n",
    "# ollama pull llama3.2:3b\n",
    "# ollama pull mistral\n",
    "\n",
    "# List available models\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Python SDK Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"‚úì Connected to Ollama\")\n",
    "    print(f\"\\nAvailable models: {len(models['models'])}\")\n",
    "    for model in models['models']:\n",
    "        print(f\"  - {model['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat\n",
    "response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'What is the capital of France?'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream tokens as they're generated\n",
    "stream = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': 'Write a short story about a robot.'}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)\n",
    "print(\"\\n\\n‚úì Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Turn Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain conversation context\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful Python tutor.'},\n",
    "    {'role': 'user', 'content': 'How do I define a function in Python?'}\n",
    "]\n",
    "\n",
    "response1 = ollama.chat(model='llama3.2:3b', messages=messages)\n",
    "print(\"Assistant:\", response1['message']['content'])\n",
    "\n",
    "# Continue conversation\n",
    "messages.append(response1['message'])\n",
    "messages.append({'role': 'user', 'content': 'Can you show me an example?'})\n",
    "\n",
    "response2 = ollama.chat(model='llama3.2:3b', messages=messages)\n",
    "print(\"\\nAssistant:\", response2['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison: Local vs Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt = \"Explain the difference between lists and tuples in Python.\"\n",
    "\n",
    "# Local inference (Ollama)\n",
    "start = time.time()\n",
    "local_response = ollama.chat(\n",
    "    model='llama3.2:3b',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "local_time = time.time() - start\n",
    "\n",
    "# Cloud inference (OpenAI)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "start = time.time()\n",
    "cloud_response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "cloud_time = time.time() - start\n",
    "\n",
    "print(f\"Local (Ollama): {local_time:.2f}s\")\n",
    "print(f\"Cloud (OpenAI): {cloud_time:.2f}s\")\n",
    "print(f\"\\nSpeedup: {local_time/cloud_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model info\n",
    "info = ollama.show('llama3.2:3b')\n",
    "print(\"Model Info:\")\n",
    "print(f\"Size: {info.get('size', 'Unknown')}\")\n",
    "print(f\"Format: {info.get('format', 'Unknown')}\")\n",
    "print(f\"Family: {info.get('family', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Local Chat Interface\n",
    "\n",
    "Create a simple chat loop that:\n",
    "1. Takes user input\n",
    "2. Maintains conversation history\n",
    "3. Uses Ollama for responses\n",
    "4. Allows switching models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "def local_chat(model: str = 'llama3.2:3b'):\n",
    "    \"\"\"Interactive chat with local model.\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    print(f\"Chat with {model} (type 'quit' to exit)\\n\")\n",
    "    \n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Uncomment to test\n",
    "# local_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- ‚úÖ Installing and configuring Ollama\n",
    "- ‚úÖ Managing local models\n",
    "- ‚úÖ Running inference with Python SDK\n",
    "- ‚úÖ Comparing local vs cloud performance\n",
    "\n",
    "## Next Steps\n",
    "- üìò Notebook 04: Prompt Injection Attacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
