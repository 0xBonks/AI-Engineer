{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Notebook 05: Chunking Strategies\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why chunking is necessary\n",
    "- Implement different chunking strategies\n",
    "- Optimize chunk size for retrieval\n",
    "- Preserve context with overlap\n",
    "- Handle different document types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Chunking Matters\n",
    "\n",
    "### The Problem:\n",
    "- Embeddings models have **token limits** (e.g., 8192 tokens)\n",
    "- Long documents lose **granularity** in single embedding\n",
    "- **Retrieval precision** suffers with large chunks\n",
    "\n",
    "### The Solution: Chunking\n",
    "- Break documents into **smaller pieces**\n",
    "- Each chunk gets its own embedding\n",
    "- Retrieve **most relevant** chunks, not entire documents\n",
    "\n",
    "### Chunking Strategies:\n",
    "1. **Fixed-size**: Split by character/token count\n",
    "2. **Sentence-based**: Split at sentence boundaries\n",
    "3. **Paragraph-based**: Split at paragraph boundaries\n",
    "4. **Semantic**: Split at topic changes\n",
    "5. **Recursive**: Try multiple strategies in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain tiktoken sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "import tiktoken\n",
    "\n",
    "# Sample long document\n",
    "sample_doc = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It has revolutionized many industries.\n",
    "\n",
    "There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\n",
    "Supervised learning uses labeled data to train models. For example, you might train a model to recognize cats by showing it many images labeled \"cat\" or \"not cat\".\n",
    "\n",
    "Unsupervised learning finds patterns in unlabeled data. Clustering algorithms are a common example of unsupervised learning.\n",
    "\n",
    "Reinforcement learning trains agents through rewards and punishments. It's commonly used in game AI and robotics.\n",
    "\n",
    "Deep learning is a subset of machine learning that uses neural networks with many layers. It has achieved remarkable success in image recognition, natural language processing, and many other domains.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Document length: {len(sample_doc)} characters\")\n",
    "print(f\"Word count: {len(sample_doc.split())} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fixed-Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character-based splitting\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on double newlines (paragraphs)\n",
    "    chunk_size=200,    # Max characters per chunk\n",
    "    chunk_overlap=20,  # Overlap between chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(sample_doc)\n",
    "\n",
    "print(f\"Fixed-size chunking: {len(char_chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(chunk[:100] + \"...\" if len(chunk) > 100 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Token-Based Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-based splitting (respects model's tokenization)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,     # Max tokens per chunk\n",
    "    chunk_overlap=10   # Overlap in tokens\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(sample_doc)\n",
    "\n",
    "print(f\"Token-based chunking: {len(token_chunks)} chunks\\n\")\n",
    "\n",
    "# Count tokens in each chunk\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "for i, chunk in enumerate(token_chunks[:3], 1):  # Show first 3\n",
    "    tokens = enc.encode(chunk)\n",
    "    print(f\"Chunk {i} ({len(tokens)} tokens):\")\n",
    "    print(chunk[:100] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recursive Chunking (Smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive splitting tries multiple separators in order\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Try in order\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(sample_doc)\n",
    "\n",
    "print(f\"Recursive chunking: {len(recursive_chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(recursive_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(f\"  [{len(chunk)} chars]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chunk Overlap\n",
    "\n",
    "Overlap preserves context across chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without overlap\n",
    "no_overlap = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ").split_text(sample_doc)\n",
    "\n",
    "with_overlap = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ").split_text(sample_doc)\n",
    "\n",
    "print(f\"Without overlap: {len(no_overlap)} chunks\")\n",
    "print(f\"With overlap: {len(with_overlap)} chunks\")\n",
    "print(\"\\nOverlap example:\")\n",
    "print(\"Chunk 1 end:\", with_overlap[0][-40:])\n",
    "print(\"Chunk 2 start:\", with_overlap[1][:40])\n",
    "print(\"\\nâ†‘ Notice the shared text between chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preserving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Create document with metadata\n",
    "doc = Document(\n",
    "    page_content=sample_doc,\n",
    "    metadata={\n",
    "        \"source\": \"ml_tutorial.txt\",\n",
    "        \"author\": \"AI Teacher\",\n",
    "        \"date\": \"2024-01-01\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Split preserving metadata\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents([doc])\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks[:2], 1):  # Show first 2\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"  Content: {chunk.page_content[:80]}...\")\n",
    "    print(f\"  Metadata: {chunk.metadata}\")\n",
    "    # Note: Each chunk gets the source metadata\n",
    "    chunk.metadata[\"chunk_index\"] = i - 1  # Add chunk position\n",
    "    print(f\"  Updated: {chunk.metadata}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimal Chunk Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunk sizes\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Query to test\n",
    "query = \"What is supervised learning?\"\n",
    "query_emb = model.encode(query)\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [50, 100, 200, 400]\n",
    "results = []\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=int(size * 0.1)  # 10% overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(sample_doc)\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    # Find best match\n",
    "    similarities = cosine_similarity([query_emb], embeddings)[0]\n",
    "    best_score = max(similarities)\n",
    "    best_chunk = chunks[np.argmax(similarities)]\n",
    "    \n",
    "    results.append({\n",
    "        \"chunk_size\": size,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"best_score\": best_score,\n",
    "        \"best_chunk\": best_chunk[:100]\n",
    "    })\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results by chunk size:\\n\")\n",
    "for r in results:\n",
    "    print(f\"Size {r['chunk_size']}: {r['num_chunks']} chunks, score {r['best_score']:.3f}\")\n",
    "    print(f\"  Best match: {r['best_chunk']}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Document-Specific Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different strategies for different document types\n",
    "\n",
    "# Code documentation\n",
    "code_doc = '''\n",
    "def calculate_sum(a, b):\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def calculate_product(a, b):\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "'''\n",
    "\n",
    "# Split on function boundaries\n",
    "code_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    separators=[\"\\ndef \", \"\\nclass \", \"\\n\\n\", \"\\n\"],\n",
    "    chunk_overlap=0\n",
    ")\n",
    "code_chunks = code_splitter.split_text(code_doc)\n",
    "print(\"Code chunks:\")\n",
    "for i, chunk in enumerate(code_chunks, 1):\n",
    "    print(f\"{i}. {chunk.strip()}\\n\")\n",
    "\n",
    "# Markdown document\n",
    "markdown_doc = '''\n",
    "# Header 1\n",
    "Content under header 1.\n",
    "\n",
    "## Header 2\n",
    "Content under header 2.\n",
    "\n",
    "### Header 3\n",
    "Content under header 3.\n",
    "'''\n",
    "\n",
    "# Split on headers\n",
    "markdown_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    separators=[\"\\n# \", \"\\n## \", \"\\n### \", \"\\n\\n\"],\n",
    "    chunk_overlap=0\n",
    ")\n",
    "md_chunks = markdown_splitter.split_text(markdown_doc)\n",
    "print(\"\\nMarkdown chunks:\")\n",
    "for i, chunk in enumerate(md_chunks, 1):\n",
    "    print(f\"{i}. {chunk.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Full Pipeline: Load â†’ Chunk â†’ Embed â†’ Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Full pipeline\n",
    "def process_document(text: str, chunk_size: int = 200):\n",
    "    \"\"\"Complete pipeline: chunk â†’ embed â†’ store.\"\"\"\n",
    "    \n",
    "    # 1. Chunk\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size * 0.1)\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    print(f\"Step 1: Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # 2. Embed\n",
    "    response = openai_client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=chunks\n",
    "    )\n",
    "    embeddings = [item.embedding for item in response.data]\n",
    "    print(f\"Step 2: Generated {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # 3. Store\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=f\"chunked_doc_{chunk_size}\"\n",
    "    )\n",
    "    collection.add(\n",
    "        documents=chunks,\n",
    "        embeddings=embeddings,\n",
    "        ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "    )\n",
    "    print(f\"Step 3: Stored in ChromaDB\")\n",
    "    \n",
    "    return collection\n",
    "\n",
    "# Process the sample document\n",
    "collection = process_document(sample_doc, chunk_size=150)\n",
    "\n",
    "# Query the collection\n",
    "query = \"Tell me about reinforcement learning\"\n",
    "query_emb = openai_client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=query\n",
    ").data[0].embedding\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_emb],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "for doc in results['documents'][0]:\n",
    "    print(f\"  â€¢ {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Smart Chunker\n",
    "\n",
    "Create an adaptive chunker that chooses the best strategy based on document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "class AdaptiveChunker:\n",
    "    \"\"\"\n",
    "    Automatically choose the best chunking strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def detect_document_type(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Detect document type (code, markdown, plain text, etc.).\n",
    "        \"\"\"\n",
    "        # TODO: Implement detection logic\n",
    "        pass\n",
    "    \n",
    "    def chunk(self, text: str, chunk_size: int = 200) -> list:\n",
    "        \"\"\"\n",
    "        Chunk text using the best strategy for its type.\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        # 1. Detect document type\n",
    "        # 2. Choose appropriate splitter\n",
    "        # 3. Return chunks\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# chunker = AdaptiveChunker()\n",
    "# chunks = chunker.chunk(your_document)\n",
    "# print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Why chunking is essential for embeddings\n",
    "- âœ… Different chunking strategies (fixed, recursive, semantic)\n",
    "- âœ… Importance of chunk overlap\n",
    "- âœ… Optimal chunk size selection\n",
    "- âœ… Document-specific chunking\n",
    "\n",
    "## Best Practices\n",
    "1. **Start with 200-500 characters** per chunk\n",
    "2. **Use 10-20% overlap** to preserve context\n",
    "3. **Recursive splitting** works well for most cases\n",
    "4. **Test different sizes** for your specific use case\n",
    "5. **Preserve metadata** through chunking process\n",
    "6. **Adapt strategy** to document type\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 06: Real-World Applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
