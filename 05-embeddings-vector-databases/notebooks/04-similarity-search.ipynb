{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Notebook 04: Similarity Search\n",
    "\n",
    "## Learning Objectives\n",
    "- Master cosine similarity and distance metrics\n",
    "- Implement k-NN search algorithms\n",
    "- Optimize search performance\n",
    "- Handle edge cases and thresholds\n",
    "- Build hybrid search (semantic + keyword)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Distance Metrics\n",
    "\n",
    "Different ways to measure similarity between vectors:\n",
    "\n",
    "### Cosine Similarity\n",
    "- Measures angle between vectors\n",
    "- Range: [-1, 1] (1 = identical, -1 = opposite)\n",
    "- **Most common** for text embeddings\n",
    "\n",
    "### Euclidean Distance\n",
    "- Straight-line distance\n",
    "- Range: [0, âˆž] (0 = identical)\n",
    "- Sensitive to magnitude\n",
    "\n",
    "### Dot Product\n",
    "- Sum of element-wise products\n",
    "- Fast to compute\n",
    "- Not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy scikit-learn sentence-transformers chromadb openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ“ Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample embeddings\n",
    "texts = [\n",
    "    \"I love machine learning\",\n",
    "    \"I enjoy studying AI\",\n",
    "    \"The weather is nice\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Cosine similarity\n",
    "cos_sim = cosine_similarity(embeddings)\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(cos_sim)\n",
    "print()\n",
    "\n",
    "# Euclidean distance\n",
    "euc_dist = euclidean_distances(embeddings)\n",
    "print(\"Euclidean Distance Matrix:\")\n",
    "print(euc_dist)\n",
    "print()\n",
    "\n",
    "# Dot product\n",
    "dot_prod = np.dot(embeddings, embeddings.T)\n",
    "print(\"Dot Product Matrix:\")\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. k-Nearest Neighbors (k-NN) Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(query_embedding, document_embeddings, k=3, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Find k most similar documents.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: Query vector\n",
    "        document_embeddings: Document vectors\n",
    "        k: Number of results\n",
    "        metric: 'cosine' or 'euclidean'\n",
    "    \n",
    "    Returns:\n",
    "        Indices of top k documents and their scores\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        scores = similarities[top_k_indices]\n",
    "    else:  # euclidean\n",
    "        distances = euclidean_distances([query_embedding], document_embeddings)[0]\n",
    "        top_k_indices = np.argsort(distances)[:k]\n",
    "        scores = distances[top_k_indices]\n",
    "    \n",
    "    return top_k_indices, scores\n",
    "\n",
    "# Test k-NN search\n",
    "documents = [\n",
    "    \"Python programming tutorial\",\n",
    "    \"Machine learning with Python\",\n",
    "    \"Cooking delicious pasta\",\n",
    "    \"Basketball game highlights\",\n",
    "    \"Deep learning neural networks\"\n",
    "]\n",
    "\n",
    "doc_embeddings = model.encode(documents)\n",
    "query = \"I want to learn AI\"\n",
    "query_emb = model.encode(query)\n",
    "\n",
    "indices, scores = knn_search(query_emb, doc_embeddings, k=3)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top 3 Results:\\n\")\n",
    "for i, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "    print(f\"{i}. [{score:.3f}] {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(query_embedding, document_embeddings, documents, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Return only documents above similarity threshold.\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "    \n",
    "    # Filter by threshold\n",
    "    above_threshold = similarities >= threshold\n",
    "    filtered_indices = np.where(above_threshold)[0]\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_indices = filtered_indices[np.argsort(similarities[filtered_indices])[::-1]]\n",
    "    \n",
    "    results = [\n",
    "        {\"document\": documents[idx], \"similarity\": similarities[idx]}\n",
    "        for idx in sorted_indices\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with different thresholds\n",
    "for threshold in [0.3, 0.5, 0.7]:\n",
    "    results = threshold_search(query_emb, doc_embeddings, documents, threshold)\n",
    "    print(f\"\\nThreshold = {threshold}: Found {len(results)} documents\")\n",
    "    for r in results:\n",
    "        print(f\"  [{r['similarity']:.3f}] {r['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Approximate Nearest Neighbors (ANN)\n",
    "\n",
    "For large datasets, exact search is slow. ANN trades accuracy for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulate large dataset\n",
    "n_documents = 10000\n",
    "embedding_dim = 384\n",
    "\n",
    "# Generate random embeddings (simulating a large corpus)\n",
    "large_corpus_embeddings = np.random.randn(n_documents, embedding_dim).astype('float32')\n",
    "\n",
    "# Normalize (important for cosine similarity)\n",
    "large_corpus_embeddings = large_corpus_embeddings / np.linalg.norm(\n",
    "    large_corpus_embeddings, axis=1, keepdims=True\n",
    ")\n",
    "\n",
    "query_vec = np.random.randn(embedding_dim).astype('float32')\n",
    "query_vec = query_vec / np.linalg.norm(query_vec)\n",
    "\n",
    "# Exact search\n",
    "start = time.time()\n",
    "exact_similarities = np.dot(large_corpus_embeddings, query_vec)\n",
    "top_k_exact = np.argsort(exact_similarities)[::-1][:5]\n",
    "exact_time = time.time() - start\n",
    "\n",
    "print(f\"Exact search on {n_documents:,} vectors: {exact_time*1000:.2f}ms\")\n",
    "print(f\"Top 5 indices: {top_k_exact}\")\n",
    "print(f\"Top 5 scores: {exact_similarities[top_k_exact]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ For very large datasets, use specialized libraries:\")\n",
    "print(\"  â€¢ FAISS (Facebook)\")\n",
    "print(\"  â€¢ Annoy (Spotify)\")\n",
    "print(\"  â€¢ HNSW (Hierarchical Navigable Small World)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hybrid Search (Semantic + Keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class HybridSearch:\n",
    "    \"\"\"\n",
    "    Combine semantic (embeddings) and keyword (TF-IDF) search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, semantic_weight=0.7):\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.keyword_weight = 1 - semantic_weight\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.documents = []\n",
    "        self.semantic_embeddings = None\n",
    "        self.keyword_matrix = None\n",
    "    \n",
    "    def index(self, documents):\n",
    "        \"\"\"Index documents for both semantic and keyword search.\"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Semantic embeddings\n",
    "        self.semantic_embeddings = model.encode(documents)\n",
    "        \n",
    "        # Keyword vectors (TF-IDF)\n",
    "        self.keyword_matrix = self.tfidf.fit_transform(documents)\n",
    "    \n",
    "    def search(self, query, k=3):\n",
    "        \"\"\"Hybrid search combining both approaches.\"\"\"\n",
    "        # Semantic search\n",
    "        query_embedding = model.encode(query)\n",
    "        semantic_scores = cosine_similarity(\n",
    "            [query_embedding],\n",
    "            self.semantic_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Keyword search\n",
    "        query_tfidf = self.tfidf.transform([query])\n",
    "        keyword_scores = cosine_similarity(\n",
    "            query_tfidf,\n",
    "            self.keyword_matrix\n",
    "        )[0]\n",
    "        \n",
    "        # Combine scores\n",
    "        hybrid_scores = (\n",
    "            self.semantic_weight * semantic_scores +\n",
    "            self.keyword_weight * keyword_scores\n",
    "        )\n",
    "        \n",
    "        # Get top k\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:k]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": hybrid_scores[idx],\n",
    "                \"semantic\": semantic_scores[idx],\n",
    "                \"keyword\": keyword_scores[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "# Test hybrid search\n",
    "corpus = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Machine learning with Python\",\n",
    "    \"Deep learning algorithms\",\n",
    "    \"Python snake in the jungle\",  # Keyword match but wrong semantic\n",
    "    \"AI and artificial intelligence\"\n",
    "]\n",
    "\n",
    "hybrid = HybridSearch(semantic_weight=0.7)\n",
    "hybrid.index(corpus)\n",
    "\n",
    "test_query = \"Python for AI\"\n",
    "results = hybrid.search(test_query, k=3)\n",
    "\n",
    "print(f\"Query: '{test_query}'\\n\")\n",
    "print(\"Hybrid Search Results:\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [score: {r['score']:.3f}] {r['document']}\")\n",
    "    print(f\"   Semantic: {r['semantic']:.3f} | Keyword: {r['keyword']:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handling Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case 1: Empty query\n",
    "try:\n",
    "    empty_emb = model.encode(\"\")\n",
    "    print(f\"Empty query embedding shape: {empty_emb.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Empty query error: {e}\")\n",
    "\n",
    "# Edge case 2: Very long text\n",
    "long_text = \"word \" * 1000\n",
    "long_emb = model.encode(long_text)\n",
    "print(f\"\\nLong text ({len(long_text)} chars) â†’ {long_emb.shape}\")\n",
    "\n",
    "# Edge case 3: No results above threshold\n",
    "unrelated_query = \"quantum physics\"\n",
    "tech_docs = [\"cooking recipe\", \"gardening tips\", \"travel guide\"]\n",
    "tech_embs = model.encode(tech_docs)\n",
    "unrelated_emb = model.encode(unrelated_query)\n",
    "\n",
    "results = threshold_search(unrelated_emb, tech_embs, tech_docs, threshold=0.7)\n",
    "print(f\"\\nUnrelated query '{unrelated_query}': {len(results)} results above 0.7\")\n",
    "\n",
    "# Edge case 4: Identical documents\n",
    "duplicate_docs = [\"same text\", \"same text\", \"different text\"]\n",
    "dup_embs = model.encode(duplicate_docs)\n",
    "dup_query_emb = model.encode(\"same text\")\n",
    "\n",
    "similarities = cosine_similarity([dup_query_emb], dup_embs)[0]\n",
    "print(f\"\\nDuplicate handling:\")\n",
    "for doc, sim in zip(duplicate_docs, similarities):\n",
    "    print(f\"  [{sim:.3f}] {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build an Advanced Search Engine\n",
    "\n",
    "Create a search engine with multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "class AdvancedSearchEngine:\n",
    "    \"\"\"\n",
    "    Advanced search with multiple strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Initialize\n",
    "        pass\n",
    "    \n",
    "    def index_documents(self, documents, metadata=None):\n",
    "        \"\"\"Index documents with metadata.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 5,\n",
    "        threshold: float = None,\n",
    "        filters: dict = None,\n",
    "        hybrid: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Search with options:\n",
    "        - k: Number of results\n",
    "        - threshold: Minimum similarity\n",
    "        - filters: Metadata filters\n",
    "        - hybrid: Use hybrid search\n",
    "        \"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# Test your engine\n",
    "# engine = AdvancedSearchEngine()\n",
    "# engine.index_documents([...])\n",
    "# results = engine.search(\"query\", hybrid=True)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Distance metrics (cosine, euclidean, dot product)\n",
    "- âœ… k-NN search implementation\n",
    "- âœ… Threshold-based filtering\n",
    "- âœ… Hybrid search (semantic + keyword)\n",
    "- âœ… Handling edge cases\n",
    "\n",
    "## Best Practices\n",
    "1. **Use cosine similarity** for text embeddings\n",
    "2. **Set appropriate thresholds** to filter noise\n",
    "3. **Combine semantic + keyword** for best results\n",
    "4. **Handle edge cases** (empty, duplicates, no results)\n",
    "5. **Use ANN libraries** for large datasets\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 05: Chunking Strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
