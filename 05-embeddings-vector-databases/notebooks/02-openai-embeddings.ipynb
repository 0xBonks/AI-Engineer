{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Notebook 02: OpenAI Embeddings\n",
    "\n",
    "## Learning Objectives\n",
    "- Use OpenAI's Embeddings API\n",
    "- Compare different embedding models\n",
    "- Implement batch processing for efficiency\n",
    "- Handle costs and rate limits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OpenAI Embedding Models\n",
    "\n",
    "OpenAI offers several embedding models:\n",
    "\n",
    "| Model | Dimensions | Cost | Use Case |\n",
    "|-------|------------|------|----------|\n",
    "| text-embedding-3-small | 1536 | $0.02/1M tokens | General purpose, cost-effective |\n",
    "| text-embedding-3-large | 3072 | $0.13/1M tokens | Higher quality, more expensive |\n",
    "| text-embedding-ada-002 | 1536 | $0.10/1M tokens | Legacy, still widely used |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai python-dotenv numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"âœ“ OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single text embedding\n",
    "text = \"Artificial intelligence is transforming the world.\"\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=text\n",
    ")\n",
    "\n",
    "embedding = response.data[0].embedding\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple texts at once\n",
    "texts = [\n",
    "    \"Machine learning is a type of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with many layers.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Computer vision allows machines to interpret images.\",\n",
    "    \"Reinforcement learning teaches agents through rewards.\"\n",
    "]\n",
    "\n",
    "# Batch API call (more efficient than individual calls)\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=texts\n",
    ")\n",
    "\n",
    "embeddings = [item.embedding for item in response.data]\n",
    "\n",
    "print(f\"Created {len(embeddings)} embeddings\")\n",
    "print(f\"Total tokens used: {response.usage.total_tokens}\")\n",
    "print(f\"Shape: ({len(embeddings)}, {len(embeddings[0])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Search with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base\n",
    "documents = [\n",
    "    \"Python is a high-level programming language.\",\n",
    "    \"JavaScript is the language of the web.\",\n",
    "    \"Machine learning models learn from data.\",\n",
    "    \"SQL is used for database queries.\",\n",
    "    \"React is a JavaScript library for building UIs.\",\n",
    "    \"TensorFlow is a machine learning framework.\",\n",
    "    \"Docker containers package applications.\",\n",
    "    \"Kubernetes orchestrates container deployments.\"\n",
    "]\n",
    "\n",
    "query = \"I want to build AI models\"\n",
    "\n",
    "# Create embeddings\n",
    "all_texts = documents + [query]\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=all_texts\n",
    ")\n",
    "\n",
    "doc_embeddings = np.array([item.embedding for item in response.data[:-1]])\n",
    "query_embedding = np.array([response.data[-1].embedding])\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Rank results\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Top Results:\\n\")\n",
    "for rank, idx in enumerate(ranked_indices[:3], 1):\n",
    "    print(f\"{rank}. [{similarities[idx]:.3f}] {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare small vs large models\n",
    "test_text = \"Quantum computing uses qubits for parallel processing.\"\n",
    "\n",
    "models = [\n",
    "    \"text-embedding-3-small\",\n",
    "    \"text-embedding-3-large\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=test_text\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    embedding = response.data[0].embedding\n",
    "    \n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(f\"  Dimensions: {len(embedding)}\")\n",
    "    print(f\"  Tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"  Time: {elapsed:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_embedding_cost(texts: list, model: str = \"text-embedding-3-small\") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate cost of embedding generation.\n",
    "    \"\"\"\n",
    "    # Pricing per 1M tokens\n",
    "    pricing = {\n",
    "        \"text-embedding-3-small\": 0.02,\n",
    "        \"text-embedding-3-large\": 0.13,\n",
    "        \"text-embedding-ada-002\": 0.10\n",
    "    }\n",
    "    \n",
    "    # Estimate tokens (rough: ~0.75 tokens per word)\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    estimated_tokens = int(total_words * 0.75)\n",
    "    \n",
    "    # Calculate cost\n",
    "    cost_per_token = pricing[model] / 1_000_000\n",
    "    estimated_cost = estimated_tokens * cost_per_token\n",
    "    \n",
    "    return {\n",
    "        \"texts\": len(texts),\n",
    "        \"estimated_tokens\": estimated_tokens,\n",
    "        \"estimated_cost_usd\": estimated_cost,\n",
    "        \"model\": model\n",
    "    }\n",
    "\n",
    "# Example: Estimate cost for a document set\n",
    "sample_docs = [f\"This is document number {i}\" for i in range(1000)]\n",
    "estimate = estimate_embedding_cost(sample_docs)\n",
    "\n",
    "print(f\"Cost Estimate:\")\n",
    "print(f\"  Documents: {estimate['texts']:,}\")\n",
    "print(f\"  Estimated Tokens: {estimate['estimated_tokens']:,}\")\n",
    "print(f\"  Estimated Cost: ${estimate['estimated_cost_usd']:.4f}\")\n",
    "print(f\"  Model: {estimate['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Caching Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Simple cache for embeddings to avoid recomputing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _hash(self, text: str, model: str) -> str:\n",
    "        \"\"\"Create cache key.\"\"\"\n",
    "        key = f\"{model}:{text}\"\n",
    "        return hashlib.md5(key.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, text: str, model: str):\n",
    "        \"\"\"Get cached embedding.\"\"\"\n",
    "        key = self._hash(text, model)\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, text: str, model: str, embedding: list):\n",
    "        \"\"\"Store embedding in cache.\"\"\"\n",
    "        key = self._hash(text, model)\n",
    "        self.cache[key] = embedding\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        return {\n",
    "            \"entries\": len(self.cache),\n",
    "            \"memory_mb\": sum(len(json.dumps(v)) for v in self.cache.values()) / 1024 / 1024\n",
    "        }\n",
    "\n",
    "# Test caching\n",
    "cache = EmbeddingCache()\n",
    "\n",
    "def get_embedding_with_cache(text: str, model: str = \"text-embedding-3-small\"):\n",
    "    \"\"\"Get embedding with caching.\"\"\"\n",
    "    # Check cache\n",
    "    cached = cache.get(text, model)\n",
    "    if cached:\n",
    "        print(\"  âœ“ Cache hit\")\n",
    "        return cached\n",
    "    \n",
    "    # Compute\n",
    "    print(\"  â†’ API call\")\n",
    "    response = client.embeddings.create(model=model, input=text)\n",
    "    embedding = response.data[0].embedding\n",
    "    \n",
    "    # Store\n",
    "    cache.set(text, model, embedding)\n",
    "    return embedding\n",
    "\n",
    "# Test\n",
    "test_text = \"Caching saves API calls and money\"\n",
    "\n",
    "print(\"First call:\")\n",
    "emb1 = get_embedding_with_cache(test_text)\n",
    "\n",
    "print(\"\\nSecond call (same text):\")\n",
    "emb2 = get_embedding_with_cache(test_text)\n",
    "\n",
    "print(f\"\\nCache stats: {cache.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Semantic Search Engine\n",
    "\n",
    "Create a simple semantic search engine using OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "class SemanticSearchEngine:\n",
    "    \"\"\"\n",
    "    A simple semantic search engine.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"text-embedding-3-small\"):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, documents: list):\n",
    "        \"\"\"Add documents to the search engine.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # 1. Create embeddings for documents\n",
    "        # 2. Store documents and embeddings\n",
    "        pass\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3):\n",
    "        \"\"\"Search for most relevant documents.\"\"\"\n",
    "        # TODO: Implement\n",
    "        # 1. Create query embedding\n",
    "        # 2. Calculate similarities\n",
    "        # 3. Return top_k results\n",
    "        pass\n",
    "\n",
    "# Test your implementation\n",
    "# engine = SemanticSearchEngine()\n",
    "# engine.add_documents([...])\n",
    "# results = engine.search(\"your query\")\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned:\n",
    "- âœ… Using OpenAI Embeddings API\n",
    "- âœ… Batch processing for efficiency\n",
    "- âœ… Comparing embedding models\n",
    "- âœ… Cost estimation and optimization\n",
    "- âœ… Caching strategies\n",
    "\n",
    "## Best Practices\n",
    "1. **Batch requests** when possible\n",
    "2. **Cache embeddings** to save costs\n",
    "3. **Use small model** unless you need highest quality\n",
    "4. **Estimate costs** before large operations\n",
    "5. **Handle rate limits** with retry logic\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Notebook 03: Vector Databases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
