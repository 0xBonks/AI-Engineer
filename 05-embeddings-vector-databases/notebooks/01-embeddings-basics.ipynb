{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Notebook 01: Embeddings Basics\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what embeddings are and why they matter\n",
    "- Learn the difference between keyword matching and semantic similarity\n",
    "- Explore embedding dimensions and properties\n",
    "- Visualize embeddings in reduced dimensions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations of text (or other data) as vectors in a high-dimensional space.\n",
    "\n",
    "### Key Properties:\n",
    "- **Semantic similarity**: Similar meanings â†’ Similar vectors\n",
    "- **Fixed dimensions**: Each embedding has the same number of dimensions (e.g., 768, 1536)\n",
    "- **Learned representations**: Created by neural networks trained on massive datasets\n",
    "\n",
    "### Why Embeddings Matter:\n",
    "- Enable semantic search (meaning-based, not just keyword)\n",
    "- Power recommendation systems\n",
    "- Enable clustering and classification\n",
    "- Foundation for RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers numpy scikit-learn matplotlib python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a small, fast model for demonstrations\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ“ Model loaded\")\n",
    "print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Your First Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example\n",
    "text = \"Machine learning is amazing!\"\n",
    "embedding = model.encode(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(f\"\\nThis text is now represented as a {len(embedding)}-dimensional vector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for multiple sentences\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"I enjoy studying AI\",\n",
    "    \"The weather is nice today\",\n",
    "    \"It's a beautiful sunny day\",\n",
    "    \"Neural networks are fascinating\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Created {len(embeddings)} embeddings\")\n",
    "print(f\"Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Display similarity between each pair\n",
    "print(\"\\nSimilarity Matrix:\")\n",
    "print(\"(1.0 = identical, 0.0 = unrelated)\\n\")\n",
    "\n",
    "for i, sent1 in enumerate(sentences):\n",
    "    for j, sent2 in enumerate(sentences):\n",
    "        if i < j:  # Only show upper triangle\n",
    "            sim = similarity_matrix[i][j]\n",
    "            print(f\"[{sim:.3f}] '{sent1}' <-> '{sent2}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keyword vs Semantic Search\n",
    "\n",
    "Let's compare traditional keyword matching with semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base\n",
    "documents = [\n",
    "    \"Python is a programming language\",\n",
    "    \"The snake slithered through the grass\",\n",
    "    \"JavaScript is used for web development\",\n",
    "    \"Coding in Python is enjoyable\",\n",
    "    \"A python is a type of reptile\"\n",
    "]\n",
    "\n",
    "query = \"I want to learn programming\"\n",
    "\n",
    "# Create embeddings\n",
    "doc_embeddings = model.encode(documents)\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "# Rank by similarity\n",
    "ranked_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Semantic Search Results (by relevance):\\n\")\n",
    "for rank, idx in enumerate(ranked_indices, 1):\n",
    "    print(f\"{rank}. [{similarities[idx]:.3f}] {documents[idx]}\")\n",
    "\n",
    "print(\"\\nNotice: Documents about programming rank higher,\")\n",
    "print(\"even though they don't contain the exact word 'learn'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Embeddings\n",
    "\n",
    "Embeddings live in high-dimensional space (384 dimensions for this model). Let's visualize them in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for themed sentences\n",
    "tech_sentences = [\n",
    "    \"Machine learning algorithms\",\n",
    "    \"Neural networks\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Deep learning models\"\n",
    "]\n",
    "\n",
    "food_sentences = [\n",
    "    \"Delicious pizza\",\n",
    "    \"Tasty pasta\",\n",
    "    \"Fresh salad\",\n",
    "    \"Gourmet burger\"\n",
    "]\n",
    "\n",
    "sports_sentences = [\n",
    "    \"Playing basketball\",\n",
    "    \"Soccer match\",\n",
    "    \"Tennis tournament\",\n",
    "    \"Swimming competition\"\n",
    "]\n",
    "\n",
    "all_sentences = tech_sentences + food_sentences + sports_sentences\n",
    "all_embeddings = model.encode(all_sentences)\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each category with different colors\n",
    "colors = ['blue', 'green', 'red']\n",
    "labels = ['Tech', 'Food', 'Sports']\n",
    "sizes = [len(tech_sentences), len(food_sentences), len(sports_sentences)]\n",
    "\n",
    "start_idx = 0\n",
    "for i, (size, color, label) in enumerate(zip(sizes, colors, labels)):\n",
    "    end_idx = start_idx + size\n",
    "    plt.scatter(\n",
    "        embeddings_2d[start_idx:end_idx, 0],\n",
    "        embeddings_2d[start_idx:end_idx, 1],\n",
    "        c=color,\n",
    "        label=label,\n",
    "        s=100,\n",
    "        alpha=0.6\n",
    "    )\n",
    "    start_idx = end_idx\n",
    "\n",
    "# Add labels to points\n",
    "for i, sent in enumerate(all_sentences):\n",
    "    plt.annotate(\n",
    "        sent[:20],\n",
    "        (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "        fontsize=8,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Embedding Visualization (384D â†’ 2D)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Similar topics cluster together in embedding space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedding Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various properties\n",
    "test_cases = [\n",
    "    (\"cat\", \"dog\", \"kitten\"),\n",
    "    (\"king\", \"queen\", \"prince\"),\n",
    "    (\"car\", \"automobile\", \"vehicle\"),\n",
    "]\n",
    "\n",
    "print(\"Embedding Relationships:\\n\")\n",
    "for word1, word2, word3 in test_cases:\n",
    "    emb1, emb2, emb3 = model.encode([word1, word2, word3])\n",
    "    \n",
    "    sim_12 = cosine_similarity([emb1], [emb2])[0][0]\n",
    "    sim_13 = cosine_similarity([emb1], [emb3])[0][0]\n",
    "    sim_23 = cosine_similarity([emb2], [emb3])[0][0]\n",
    "    \n",
    "    print(f\"{word1} - {word2}: {sim_12:.3f}\")\n",
    "    print(f\"{word1} - {word3}: {sim_13:.3f}\")\n",
    "    print(f\"{word2} - {word3}: {sim_23:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise: Find Similar Sentences\n",
    "\n",
    "Complete the following exercise to practice working with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this exercise\n",
    "\n",
    "def find_most_similar(query: str, candidates: list, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Find the most similar sentences to a query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        candidates: List of candidate sentences\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of (sentence, similarity_score) tuples\n",
    "    \"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Create embeddings for query and candidates\n",
    "    # 2. Calculate similarities\n",
    "    # 3. Sort and return top_k results\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "knowledge_base = [\n",
    "    \"The Earth orbits around the Sun\",\n",
    "    \"Python is a versatile programming language\",\n",
    "    \"The Mona Lisa was painted by Leonardo da Vinci\",\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"The capital of France is Paris\",\n",
    "    \"JavaScript runs in web browsers\",\n",
    "    \"Mount Everest is the tallest mountain\",\n",
    "    \"Neural networks mimic brain structure\"\n",
    "]\n",
    "\n",
    "test_query = \"Tell me about artificial intelligence\"\n",
    "\n",
    "# results = find_most_similar(test_query, knowledge_base)\n",
    "# for sent, score in results:\n",
    "#     print(f\"[{score:.3f}] {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- âœ… What embeddings are and why they matter\n",
    "- âœ… How to create embeddings with Sentence Transformers\n",
    "- âœ… The difference between keyword and semantic search\n",
    "- âœ… How to calculate semantic similarity\n",
    "- âœ… How to visualize embeddings\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Embeddings capture meaning**, not just words\n",
    "2. **Similar meanings = similar vectors** in embedding space\n",
    "3. **Cosine similarity** measures how related two embeddings are\n",
    "4. **Semantic search** finds relevant content by meaning, not keywords\n",
    "\n",
    "## Next Steps\n",
    "- ðŸ“˜ Proceed to Notebook 02: OpenAI Embeddings\n",
    "- ðŸ”— Learn about [Sentence Transformers](https://www.sbert.net/)\n",
    "- ðŸ“š Read about [Word2Vec and embeddings history](https://en.wikipedia.org/wiki/Word2vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
