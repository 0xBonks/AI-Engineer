{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05 - Notebook 06: Real-World Applications\n",
    "\n",
    "## Learning Objectives\n",
    "- Build a semantic search engine\n",
    "- Create a document classification system\n",
    "- Implement a recommendation engine\n",
    "- Build a question-answering system\n",
    "- Combine embeddings with LLMs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "\n",
    "We'll build four real-world applications:\n",
    "\n",
    "1. **Semantic Search Engine**: Find documents by meaning\n",
    "2. **Text Classification**: Categorize documents automatically\n",
    "3. **Recommendation System**: Suggest similar content\n",
    "4. **Q&A System**: Answer questions from a knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb openai sentence-transformers scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "local_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "print(\"âœ“ All systems ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application 1: Semantic Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearchEngine:\n",
    "    \"\"\"Production-ready semantic search.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"search_engine\"):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(collection_name)\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.doc_count = 0\n",
    "    \n",
    "    def index_documents(self, documents: list, metadata: list = None):\n",
    "        \"\"\"Add documents to the search index.\"\"\"\n",
    "        # Generate embeddings\n",
    "        response = self.openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=documents\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        \n",
    "        # Add to vector DB\n",
    "        ids = [f\"doc_{self.doc_count + i}\" for i in range(len(documents))]\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=ids,\n",
    "            metadatas=metadata if metadata else [{} for _ in documents]\n",
    "        )\n",
    "        \n",
    "        self.doc_count += len(documents)\n",
    "        return f\"Indexed {len(documents)} documents (total: {self.doc_count})\"\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, filters: dict = None):\n",
    "        \"\"\"Search for relevant documents.\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_response = self.openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=query\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=filters\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        return [\n",
    "            {\n",
    "                \"document\": doc,\n",
    "                \"distance\": dist,\n",
    "                \"metadata\": meta\n",
    "            }\n",
    "            for doc, dist, meta in zip(\n",
    "                results['documents'][0],\n",
    "                results['distances'][0],\n",
    "                results['metadatas'][0]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "# Demo the search engine\n",
    "engine = SemanticSearchEngine()\n",
    "\n",
    "# Index sample documents\n",
    "docs = [\n",
    "    \"Python is a versatile programming language\",\n",
    "    \"Machine learning enables AI applications\",\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"Neural networks process data like human brains\",\n",
    "    \"Climate change affects global temperatures\"\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"category\": \"tech\", \"year\": 2024},\n",
    "    {\"category\": \"tech\", \"year\": 2024},\n",
    "    {\"category\": \"travel\", \"year\": 2024},\n",
    "    {\"category\": \"tech\", \"year\": 2024},\n",
    "    {\"category\": \"science\", \"year\": 2024}\n",
    "]\n",
    "\n",
    "print(engine.index_documents(docs, metadata))\n",
    "\n",
    "# Search\n",
    "results = engine.search(\"Tell me about AI\", n_results=3)\n",
    "\n",
    "print(\"\\nSearch Results:\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. [{r['distance']:.3f}] {r['document']}\")\n",
    "    print(f\"   Category: {r['metadata']['category']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application 2: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class EmbeddingClassifier:\n",
    "    \"\"\"Classify text using embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.classifier = LogisticRegression(max_iter=1000)\n",
    "        self.is_trained = False\n",
    "    \n",
    "    def train(self, texts: list, labels: list):\n",
    "        \"\"\"Train the classifier.\"\"\"\n",
    "        # Generate embeddings\n",
    "        embeddings = self.model.encode(texts)\n",
    "        \n",
    "        # Train classifier\n",
    "        self.classifier.fit(embeddings, labels)\n",
    "        self.is_trained = True\n",
    "        \n",
    "        return f\"Trained on {len(texts)} examples\"\n",
    "    \n",
    "    def predict(self, text: str):\n",
    "        \"\"\"Classify a text.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.model.encode([text])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.classifier.predict(embedding)[0]\n",
    "        probabilities = self.classifier.predict_proba(embedding)[0]\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"confidence\": max(probabilities)\n",
    "        }\n",
    "\n",
    "# Demo classification\n",
    "classifier = EmbeddingClassifier()\n",
    "\n",
    "# Training data\n",
    "training_texts = [\n",
    "    \"Python is great for data science\",\n",
    "    \"Machine learning models learn from data\",\n",
    "    \"The recipe requires three eggs\",\n",
    "    \"Bake at 350 degrees for 30 minutes\",\n",
    "    \"The soccer match ended in a draw\",\n",
    "    \"Basketball is played with five players\",\n",
    "    \"Neural networks have multiple layers\",\n",
    "    \"Add salt and pepper to taste\",\n",
    "    \"The team scored in overtime\",\n",
    "    \"Deep learning uses GPUs for training\"\n",
    "]\n",
    "\n",
    "training_labels = [\n",
    "    \"tech\", \"tech\", \"cooking\", \"cooking\", \"sports\",\n",
    "    \"sports\", \"tech\", \"cooking\", \"sports\", \"tech\"\n",
    "]\n",
    "\n",
    "print(classifier.train(training_texts, training_labels))\n",
    "\n",
    "# Test classification\n",
    "test_texts = [\n",
    "    \"I love programming in Python\",\n",
    "    \"This pasta dish is delicious\",\n",
    "    \"The football game was exciting\"\n",
    "]\n",
    "\n",
    "print(\"\\nClassification Results:\\n\")\n",
    "for text in test_texts:\n",
    "    result = classifier.predict(text)\n",
    "    print(f\"'{text}'\")\n",
    "    print(f\"  â†’ {result['prediction']} ({result['confidence']:.2f} confidence)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Application 3: Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationEngine:\n",
    "    \"\"\"Content-based recommendations using embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.items = []\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def add_items(self, items: list, descriptions: list):\n",
    "        \"\"\"Add items to the recommendation pool.\"\"\"\n",
    "        self.items = [\n",
    "            {\"name\": item, \"description\": desc}\n",
    "            for item, desc in zip(items, descriptions)\n",
    "        ]\n",
    "        \n",
    "        # Generate embeddings from descriptions\n",
    "        self.embeddings = self.model.encode(descriptions)\n",
    "        \n",
    "        return f\"Added {len(items)} items\"\n",
    "    \n",
    "    def recommend(self, item_name: str = None, description: str = None, n: int = 3):\n",
    "        \"\"\"Get recommendations based on item or description.\"\"\"\n",
    "        if item_name:\n",
    "            # Find item's index\n",
    "            item_idx = next(\n",
    "                (i for i, item in enumerate(self.items) if item[\"name\"] == item_name),\n",
    "                None\n",
    "            )\n",
    "            if item_idx is None:\n",
    "                return []\n",
    "            \n",
    "            query_embedding = self.embeddings[item_idx]\n",
    "        \n",
    "        elif description:\n",
    "            # Generate embedding from description\n",
    "            query_embedding = self.model.encode([description])[0]\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Provide either item_name or description\")\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(\n",
    "            [query_embedding],\n",
    "            self.embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Get top N (excluding the query item itself)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        if item_name:\n",
    "            top_indices = top_indices[1:n+1]  # Skip first (same item)\n",
    "        else:\n",
    "            top_indices = top_indices[:n]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"name\": self.items[idx][\"name\"],\n",
    "                \"description\": self.items[idx][\"description\"],\n",
    "                \"similarity\": similarities[idx]\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "# Demo recommendations\n",
    "recommender = RecommendationEngine()\n",
    "\n",
    "# Add movies\n",
    "movies = [\n",
    "    \"The Matrix\",\n",
    "    \"Inception\",\n",
    "    \"Interstellar\",\n",
    "    \"The Notebook\",\n",
    "    \"Titanic\",\n",
    "    \"Die Hard\",\n",
    "    \"Blade Runner\"\n",
    "]\n",
    "\n",
    "descriptions = [\n",
    "    \"A sci-fi thriller about simulated reality and AI\",\n",
    "    \"Mind-bending sci-fi about dreams within dreams\",\n",
    "    \"Space exploration and time dilation sci-fi epic\",\n",
    "    \"Romantic drama about love and memory\",\n",
    "    \"Romantic tragedy set on a doomed ocean liner\",\n",
    "    \"Action-packed thriller with explosive sequences\",\n",
    "    \"Dystopian sci-fi noir about artificial humans\"\n",
    "]\n",
    "\n",
    "print(recommender.add_items(movies, descriptions))\n",
    "\n",
    "# Get recommendations based on a movie\n",
    "print(\"\\nIf you liked 'The Matrix', you might also like:\\n\")\n",
    "recs = recommender.recommend(item_name=\"The Matrix\", n=3)\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"{i}. {rec['name']} ({rec['similarity']:.3f})\")\n",
    "    print(f\"   {rec['description']}\\n\")\n",
    "\n",
    "# Get recommendations based on a description\n",
    "print(\"Movies matching 'romantic love story':\\n\")\n",
    "recs = recommender.recommend(description=\"romantic love story\", n=2)\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"{i}. {rec['name']} ({rec['similarity']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application 4: Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QASystem:\n",
    "    \"\"\"Question-answering with embeddings + LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chroma = chromadb.Client()\n",
    "        self.collection = self.chroma.create_collection(\"qa_knowledge\")\n",
    "        self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def add_knowledge(self, documents: list):\n",
    "        \"\"\"Add documents to the knowledge base.\"\"\"\n",
    "        # Generate embeddings\n",
    "        response = self.openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=documents\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        \n",
    "        # Store in vector DB\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=[f\"kb_{i}\" for i in range(len(documents))]\n",
    "        )\n",
    "        \n",
    "        return f\"Added {len(documents)} documents to knowledge base\"\n",
    "    \n",
    "    def ask(self, question: str, n_context: int = 3) -> str:\n",
    "        \"\"\"Answer a question using the knowledge base.\"\"\"\n",
    "        # 1. Find relevant context\n",
    "        query_response = self.openai_client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=question\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_context\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n\".join(results['documents'][0])\n",
    "        \n",
    "        # 2. Generate answer using LLM\n",
    "        prompt = f\"\"\"Answer the question based on the context below. If the answer cannot be found in the context, say \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        chat_response = self.openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        answer = chat_response.choices[0].message.content\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": results['documents'][0][:2]  # Show top 2 sources\n",
    "        }\n",
    "\n",
    "# Demo Q&A system\n",
    "qa = QASystem()\n",
    "\n",
    "# Add knowledge\n",
    "knowledge = [\n",
    "    \"Python was created by Guido van Rossum in 1991.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Neural networks are inspired by biological neurons.\",\n",
    "    \"Python is known for its simple and readable syntax.\"\n",
    "]\n",
    "\n",
    "print(qa.add_knowledge(knowledge))\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What is the capital of Spain?\"  # Not in knowledge base\n",
    "]\n",
    "\n",
    "print(\"\\nQ&A Demo:\\n\")\n",
    "for q in questions:\n",
    "    result = qa.ask(q)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {result['sources'][0][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Example: Document Search + Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntelligentDocumentAssistant:\n",
    "    \"\"\"Combines search and chat for document interaction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.qa_system = QASystem()\n",
    "    \n",
    "    def load_documents(self, documents: list):\n",
    "        \"\"\"Load documents into the system.\"\"\"\n",
    "        return self.qa_system.add_knowledge(documents)\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        \"\"\"Chat about the documents.\"\"\"\n",
    "        result = self.qa_system.ask(user_message)\n",
    "        return result['answer']\n",
    "\n",
    "# Demo the complete system\n",
    "assistant = IntelligentDocumentAssistant()\n",
    "\n",
    "# Load company handbook\n",
    "handbook = [\n",
    "    \"Company Policy: All employees must clock in by 9 AM.\",\n",
    "    \"Vacation Policy: Employees get 15 days of paid vacation per year.\",\n",
    "    \"Remote Work: Employees can work remotely up to 2 days per week.\",\n",
    "    \"Health Benefits: Full health insurance is provided after 90 days.\",\n",
    "    \"Dress Code: Business casual attire is required in the office.\"\n",
    "]\n",
    "\n",
    "print(assistant.load_documents(handbook))\n",
    "print(\"\\nDocument Assistant Ready!\\n\")\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation = [\n",
    "    \"What time do I need to arrive at work?\",\n",
    "    \"How many vacation days do I get?\",\n",
    "    \"Can I work from home?\"\n",
    "]\n",
    "\n",
    "for question in conversation:\n",
    "    answer = assistant.chat(question)\n",
    "    print(f\"You: {question}\")\n",
    "    print(f\"Assistant: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned to build:\n",
    "- âœ… Semantic search engines\n",
    "- âœ… Text classifiers with embeddings\n",
    "- âœ… Recommendation systems\n",
    "- âœ… Q&A systems (RAG pattern)\n",
    "- âœ… Complete document assistants\n",
    "\n",
    "## Key Patterns\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation)\n",
    "1. **Retrieve** relevant context with embeddings\n",
    "2. **Augment** prompt with retrieved context\n",
    "3. **Generate** answer with LLM\n",
    "\n",
    "### Embedding Pipeline\n",
    "1. **Chunk** documents appropriately\n",
    "2. **Embed** chunks with consistent model\n",
    "3. **Store** in vector database\n",
    "4. **Query** with semantic search\n",
    "5. **Post-process** results\n",
    "\n",
    "## Production Considerations\n",
    "1. **Error handling**: API failures, rate limits\n",
    "2. **Caching**: Store embeddings to reduce costs\n",
    "3. **Monitoring**: Track performance and costs\n",
    "4. **Updates**: Refresh embeddings when documents change\n",
    "5. **Security**: Sanitize inputs, control access\n",
    "\n",
    "## Next Steps\n",
    "- ðŸš€ Build your own application\n",
    "- ðŸ“˜ Proceed to Module 06: RAG Systems\n",
    "- ðŸ”— Explore [Pinecone](https://www.pinecone.io/) for production scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Module 05 Complete!\n",
    "\n",
    "You've mastered embeddings and vector databases! You can now:\n",
    "- Create and work with embeddings\n",
    "- Use vector databases effectively\n",
    "- Build production applications\n",
    "- Implement RAG patterns\n",
    "\n",
    "**Congratulations!** You're ready for advanced RAG in Module 06."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
