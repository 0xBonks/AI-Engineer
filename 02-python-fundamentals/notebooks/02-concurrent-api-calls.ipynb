{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrent API Calls\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you'll learn to make concurrent API calls efficiently - a critical skill for AI engineering.\n",
    "\n",
    "**Real-world scenario:** Process 100 text snippets with OpenAI API\n",
    "- Sequential: ~500 seconds (8+ minutes)\n",
    "- Concurrent: ~50 seconds (under 1 minute)\n",
    "- **10x speedup!**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Make concurrent HTTP requests with `httpx`\n",
    "2. Handle errors in concurrent scenarios\n",
    "3. Measure and compare performance\n",
    "4. Apply to real AI API calls (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import List\n",
    "import httpx  # Async HTTP client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequential vs Concurrent Requests\n",
    "\n",
    "### Sequential Requests (Slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sync(url: str) -> dict:\n",
    "    \"\"\"Synchronous HTTP GET request.\"\"\"\n",
    "    with httpx.Client() as client:\n",
    "        response = client.get(url, timeout=10.0)\n",
    "        return response.json()\n",
    "\n",
    "# Test with JSONPlaceholder API\n",
    "urls = [\n",
    "    \"https://jsonplaceholder.typicode.com/posts/1\",\n",
    "    \"https://jsonplaceholder.typicode.com/posts/2\",\n",
    "    \"https://jsonplaceholder.typicode.com/posts/3\",\n",
    "    \"https://jsonplaceholder.typicode.com/posts/4\",\n",
    "    \"https://jsonplaceholder.typicode.com/posts/5\",\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "results = [fetch_sync(url) for url in urls]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Sequential: {elapsed:.2f}s for {len(urls)} requests\")\n",
    "print(f\"First result: {results[0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent Requests (Fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_async(url: str) -> dict:\n",
    "    \"\"\"Asynchronous HTTP GET request.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, timeout=10.0)\n",
    "        return response.json()\n",
    "\n",
    "start = time.time()\n",
    "results = await asyncio.gather(*[fetch_async(url) for url in urls])\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Concurrent: {elapsed:.2f}s for {len(urls)} requests\")\n",
    "print(f\"First result: {results[0]['title']}\")\n",
    "print(f\"\\nSpeedup: {elapsed:.2f}s vs previous (approximately {len(urls)/elapsed:.1f}x faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Handling in Concurrent Requests\n",
    "\n",
    "When making concurrent requests, errors become more likely. Handle them gracefully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_with_error_handling(url: str) -> dict:\n",
    "    \"\"\"Fetch with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url, timeout=10.0)\n",
    "            response.raise_for_status()  # Raise error for 4xx/5xx\n",
    "            return {\"success\": True, \"data\": response.json(), \"url\": url}\n",
    "    \n",
    "    except httpx.TimeoutException:\n",
    "        return {\"success\": False, \"error\": \"timeout\", \"url\": url}\n",
    "    \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        return {\"success\": False, \"error\": f\"HTTP {e.response.status_code}\", \"url\": url}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"url\": url}\n",
    "\n",
    "# Test with mix of valid and invalid URLs\n",
    "test_urls = [\n",
    "    \"https://jsonplaceholder.typicode.com/posts/1\",\n",
    "    \"https://jsonplaceholder.typicode.com/posts/999999\",  # 404\n",
    "    \"https://jsonplaceholder.typicode.com/posts/2\",\n",
    "]\n",
    "\n",
    "results = await asyncio.gather(*[fetch_with_error_handling(url) for url in test_urls])\n",
    "\n",
    "for result in results:\n",
    "    if result[\"success\"]:\n",
    "        print(f\"✓ {result['url']}: {result['data']['title'][:30]}...\")\n",
    "    else:\n",
    "        print(f\"✗ {result['url']}: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Processing Pattern\n",
    "\n",
    "Process large lists of items efficiently with progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_fetch(urls: List[str], batch_size: int = 10) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Fetch URLs in batches to avoid overwhelming the server.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URLs to fetch\n",
    "        batch_size: Number of concurrent requests per batch\n",
    "    \n",
    "    Returns:\n",
    "        List of results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        batch = urls[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(urls)-1)//batch_size + 1}...\")\n",
    "        \n",
    "        batch_results = await asyncio.gather(\n",
    "            *[fetch_with_error_handling(url) for url in batch]\n",
    "        )\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with 15 URLs in batches of 5\n",
    "urls = [f\"https://jsonplaceholder.typicode.com/posts/{i}\" for i in range(1, 16)]\n",
    "\n",
    "start = time.time()\n",
    "results = await batch_fetch(urls, batch_size=5)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "successful = sum(1 for r in results if r[\"success\"])\n",
    "print(f\"\\nCompleted: {successful}/{len(results)} successful in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real Example: OpenAI API Calls\n",
    "\n",
    "**Note:** This example shows the pattern. You'll need a valid API key to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize async client\n",
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def generate_completion(prompt: str) -> dict:\n",
    "    \"\"\"\n",
    "    Generate completion for a single prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text prompt\n",
    "    \n",
    "    Returns:\n",
    "        Result dictionary with success status and content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"tokens\": response.usage.total_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"prompt\": prompt,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Example prompts\n",
    "prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain async programming.\",\n",
    "    \"What are the benefits of AI?\",\n",
    "    \"How does machine learning work?\",\n",
    "    \"What is a neural network?\"\n",
    "]\n",
    "\n",
    "# Only run if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Processing prompts concurrently...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    results = await asyncio.gather(*[generate_completion(p) for p in prompts])\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nCompleted {len(prompts)} prompts in {elapsed:.2f}s\")\n",
    "    \n",
    "    for result in results:\n",
    "        if result[\"success\"]:\n",
    "            print(f\"\\n✓ Prompt: {result['prompt']}\")\n",
    "            print(f\"  Response: {result['response'][:100]}...\")\n",
    "            print(f\"  Tokens: {result['tokens']}\")\n",
    "        else:\n",
    "            print(f\"\\n✗ Prompt: {result['prompt']}\")\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "else:\n",
    "    print(\"⚠ Skipping OpenAI example (no API key)\")\n",
    "    print(\"Set OPENAI_API_KEY environment variable to run this example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's measure the actual speedup with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def performance_test(num_requests: int, concurrent: bool = True):\n",
    "    \"\"\"Test performance of sequential vs concurrent requests.\"\"\"\n",
    "    urls = [f\"https://jsonplaceholder.typicode.com/posts/{i % 100 + 1}\" for i in range(num_requests)]\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    if concurrent:\n",
    "        results = await asyncio.gather(*[fetch_async(url) for url in urls])\n",
    "    else:\n",
    "        results = []\n",
    "        for url in urls:\n",
    "            result = await fetch_async(url)\n",
    "            results.append(result)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return elapsed, len(results)\n",
    "\n",
    "# Test with different batch sizes\n",
    "test_sizes = [5, 10, 20]\n",
    "\n",
    "print(\"Performance Comparison:\\n\")\n",
    "print(f\"{'Requests':<12} {'Sequential':<15} {'Concurrent':<15} {'Speedup':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in test_sizes:\n",
    "    seq_time, _ = await performance_test(size, concurrent=False)\n",
    "    conc_time, _ = await performance_test(size, concurrent=True)\n",
    "    speedup = seq_time / conc_time\n",
    "    \n",
    "    print(f\"{size:<12} {seq_time:<15.2f} {conc_time:<15.2f} {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "### ✅ Do:\n",
    "\n",
    "1. **Use connection pooling** (httpx.AsyncClient reuses connections)\n",
    "2. **Handle errors individually** (don't let one failure stop all)\n",
    "3. **Set timeouts** (prevent hanging forever)\n",
    "4. **Respect rate limits** (use semaphores, covered in next notebook)\n",
    "5. **Track progress** (for long-running batches)\n",
    "\n",
    "### ❌ Don't:\n",
    "\n",
    "1. **Make unlimited concurrent requests** (overwhelm servers)\n",
    "2. **Ignore errors** (handle them gracefully)\n",
    "3. **Use sync libraries** (blocks the event loop)\n",
    "4. **Skip timeouts** (requests can hang)\n",
    "5. **Forget to close clients** (use async context managers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practice Exercise\n",
    "\n",
    "Create a function that fetches user data and their posts concurrently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_user_with_posts(user_id: int) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch user data and all their posts concurrently.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with user info and posts\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    # 1. Fetch user from: https://jsonplaceholder.typicode.com/users/{user_id}\n",
    "    # 2. Fetch user's posts from: https://jsonplaceholder.typicode.com/posts?userId={user_id}\n",
    "    # 3. Use asyncio.gather to fetch both concurrently\n",
    "    # 4. Return combined result\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# result = await fetch_user_with_posts(1)\n",
    "# print(f\"User: {result['user']['name']}\")\n",
    "# print(f\"Posts: {len(result['posts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **httpx.AsyncClient for async HTTP requests**\n",
    "2. **asyncio.gather() for concurrent execution**\n",
    "3. **Always handle errors in concurrent code**\n",
    "4. **Batch processing prevents overwhelming servers**\n",
    "5. **10x+ speedups common for I/O-bound operations**\n",
    "\n",
    "### Real-World Impact:\n",
    "\n",
    "- Process 100 prompts: 500s → 50s\n",
    "- Check 50 endpoints: 100s → 10s\n",
    "- Fetch 1000 documents: hours → minutes\n",
    "\n",
    "### Next Notebook:\n",
    "\n",
    "**Async Patterns for AI** - Rate limiting, semaphores, timeouts, and production patterns\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [httpx async documentation](https://www.python-httpx.org/async/)\n",
    "- [OpenAI async examples](https://github.com/openai/openai-python)\n",
    "- [asyncio patterns](https://www.roguelynn.com/words/asyncio-we-did-it-wrong/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
