{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Patterns for AI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Advanced async patterns essential for production AI systems:\n",
    "- **Rate limiting** - Respect API quotas\n",
    "- **Semaphores** - Control concurrency\n",
    "- **Timeouts** - Handle slow responses\n",
    "- **Retries** - Handle transient failures\n",
    "- **Circuit breakers** - Fail fast when service is down\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Implement rate limiting with semaphores\n",
    "2. Handle timeouts gracefully\n",
    "3. Build retry logic with exponential backoff\n",
    "4. Track progress in long-running operations\n",
    "5. Combine patterns for production-ready code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import List, Optional\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rate Limiting with Semaphores\n",
    "\n",
    "**Problem:** Making 1000 concurrent API calls overwhelms the server.\n",
    "\n",
    "**Solution:** Use semaphores to limit concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_with_semaphore(url: str, semaphore: asyncio.Semaphore) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch URL with semaphore to limit concurrency.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to fetch\n",
    "        semaphore: Semaphore limiting concurrent requests\n",
    "    \"\"\"\n",
    "    async with semaphore:  # Acquire semaphore (blocks if limit reached)\n",
    "        print(f\"Fetching {url[-20:]}...\")\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(url, timeout=10.0)\n",
    "            await asyncio.sleep(0.1)  # Simulate processing\n",
    "            return response.json()\n",
    "\n",
    "async def rate_limited_fetch_demo():\n",
    "    \"\"\"Demonstrate rate limiting with different concurrency levels.\"\"\"\n",
    "    urls = [f\"https://jsonplaceholder.typicode.com/posts/{i}\" for i in range(1, 21)]\n",
    "    \n",
    "    # Test with different limits\n",
    "    for limit in [20, 5, 2]:\n",
    "        print(f\"\\n=== Concurrency limit: {limit} ===\")\n",
    "        semaphore = asyncio.Semaphore(limit)\n",
    "        \n",
    "        start = time.time()\n",
    "        results = await asyncio.gather(\n",
    "            *[fetch_with_semaphore(url, semaphore) for url in urls]\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"Completed in {elapsed:.2f}s with {limit} concurrent requests\\n\")\n",
    "\n",
    "await rate_limited_fetch_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `Semaphore(5)` allows max 5 concurrent operations\n",
    "- When limit reached, other tasks wait\n",
    "- Prevents overwhelming the API\n",
    "- Essential for respecting rate limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Timeouts and Graceful Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_with_timeout(url: str, timeout_seconds: float = 5.0) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch with timeout, return error if too slow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await asyncio.wait_for(\n",
    "                client.get(url, timeout=timeout_seconds),\n",
    "                timeout=timeout_seconds\n",
    "            )\n",
    "            return {\"success\": True, \"data\": response.json()}\n",
    "    \n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"success\": False, \"error\": \"timeout\", \"url\": url}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"url\": url}\n",
    "\n",
    "# Test with various timeouts\n",
    "url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "\n",
    "for timeout in [10.0, 0.001]:  # 0.001s will definitely timeout\n",
    "    print(f\"\\nTesting with {timeout}s timeout...\")\n",
    "    result = await fetch_with_timeout(url, timeout)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"âœ“ Success: {result['data']['title'][:30]}...\")\n",
    "    else:\n",
    "        print(f\"âœ— Failed: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retry Logic with Exponential Backoff\n",
    "\n",
    "**Pattern:** Retry failed requests with increasing delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_with_retry(\n",
    "    url: str,\n",
    "    max_retries: int = 3,\n",
    "    base_delay: float = 1.0\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fetch with exponential backoff retry.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to fetch\n",
    "        max_retries: Maximum retry attempts\n",
    "        base_delay: Initial delay between retries (doubles each time)\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, timeout=10.0)\n",
    "                response.raise_for_status()\n",
    "                return {\"success\": True, \"data\": response.json(), \"attempts\": attempt + 1}\n",
    "        \n",
    "        except (httpx.HTTPError, httpx.TimeoutException) as e:\n",
    "            if attempt < max_retries:\n",
    "                delay = base_delay * (2 ** attempt)  # Exponential backoff\n",
    "                print(f\"Attempt {attempt + 1} failed, retrying in {delay}s...\")\n",
    "                await asyncio.sleep(delay)\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"attempts\": attempt + 1\n",
    "                }\n",
    "\n",
    "# Test with URL that might be flaky\n",
    "result = await fetch_with_retry(\"https://jsonplaceholder.typicode.com/posts/1\")\n",
    "print(f\"\\nResult: {result['success']} after {result['attempts']} attempt(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Progress Tracking for Long Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_with_progress(items: List[str], semaphore: asyncio.Semaphore):\n",
    "    \"\"\"\n",
    "    Process items with progress tracking.\n",
    "    \"\"\"\n",
    "    total = len(items)\n",
    "    completed = 0\n",
    "    \n",
    "    async def process_item(item: str):\n",
    "        nonlocal completed\n",
    "        async with semaphore:\n",
    "            # Simulate processing\n",
    "            await asyncio.sleep(0.5)\n",
    "            completed += 1\n",
    "            \n",
    "            # Print progress every 10%\n",
    "            progress = (completed / total) * 100\n",
    "            if completed % max(1, total // 10) == 0 or completed == total:\n",
    "                print(f\"Progress: {completed}/{total} ({progress:.1f}%)\")\n",
    "            \n",
    "            return f\"Processed {item}\"\n",
    "    \n",
    "    results = await asyncio.gather(*[process_item(item) for item in items])\n",
    "    return results\n",
    "\n",
    "# Test with 50 items, max 10 concurrent\n",
    "items = [f\"item-{i}\" for i in range(50)]\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "print(\"Processing 50 items...\\n\")\n",
    "start = time.time()\n",
    "results = await process_with_progress(items, semaphore)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nCompleted in {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production-Ready Pattern: Combining Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncAPIClient:\n",
    "    \"\"\"\n",
    "    Production-ready async API client with:\n",
    "    - Rate limiting\n",
    "    - Retry logic\n",
    "    - Timeout handling\n",
    "    - Progress tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        max_concurrent: int = 10,\n",
    "        max_retries: int = 3,\n",
    "        timeout: float = 30.0\n",
    "    ):\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        self.max_retries = max_retries\n",
    "        self.timeout = timeout\n",
    "        self.completed = 0\n",
    "        self.failed = 0\n",
    "    \n",
    "    async def fetch(self, url: str) -> dict:\n",
    "        \"\"\"Fetch single URL with all protections.\"\"\"\n",
    "        async with self.semaphore:\n",
    "            for attempt in range(self.max_retries + 1):\n",
    "                try:\n",
    "                    async with httpx.AsyncClient() as client:\n",
    "                        response = await asyncio.wait_for(\n",
    "                            client.get(url, timeout=self.timeout),\n",
    "                            timeout=self.timeout\n",
    "                        )\n",
    "                        response.raise_for_status()\n",
    "                        self.completed += 1\n",
    "                        return {\"success\": True, \"data\": response.json()}\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if attempt < self.max_retries:\n",
    "                        delay = 1.0 * (2 ** attempt)\n",
    "                        await asyncio.sleep(delay)\n",
    "                    else:\n",
    "                        self.failed += 1\n",
    "                        return {\"success\": False, \"error\": str(e)}\n",
    "    \n",
    "    async def fetch_all(self, urls: List[str]) -> List[dict]:\n",
    "        \"\"\"Fetch multiple URLs with progress tracking.\"\"\"\n",
    "        print(f\"Fetching {len(urls)} URLs...\")\n",
    "        print(f\"Settings: max_concurrent={self.semaphore._value}, \"\n",
    "              f\"max_retries={self.max_retries}, timeout={self.timeout}s\\n\")\n",
    "        \n",
    "        start = time.time()\n",
    "        results = await asyncio.gather(*[self.fetch(url) for url in urls])\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(f\"\\nCompleted: {self.completed} successful, {self.failed} failed\")\n",
    "        print(f\"Time: {elapsed:.2f}s ({len(urls)/elapsed:.1f} req/s)\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the production client\n",
    "client = AsyncAPIClient(max_concurrent=5, max_retries=2, timeout=10.0)\n",
    "urls = [f\"https://jsonplaceholder.typicode.com/posts/{i}\" for i in range(1, 21)]\n",
    "\n",
    "results = await client.fetch_all(urls)\n",
    "\n",
    "# Show sample results\n",
    "successful = [r for r in results if r[\"success\"]]\n",
    "print(f\"\\nFirst successful result: {successful[0]['data']['title'][:40]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World OpenAI Example\n",
    "\n",
    "Apply all patterns to OpenAI API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "class OpenAIBatchProcessor:\n",
    "    \"\"\"\n",
    "    Process multiple prompts with OpenAI API efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, max_concurrent: int = 5):\n",
    "        self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    async def generate(self, prompt: str, max_retries: int = 3) -> dict:\n",
    "        \"\"\"Generate completion with retry logic.\"\"\"\n",
    "        async with self.semaphore:\n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    response = await self.client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        max_tokens=100\n",
    "                    )\n",
    "                    \n",
    "                    self.total_tokens += response.usage.total_tokens\n",
    "                    \n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"response\": response.choices[0].message.content,\n",
    "                        \"tokens\": response.usage.total_tokens\n",
    "                    }\n",
    "                \n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries:\n",
    "                        delay = 2.0 * (2 ** attempt)\n",
    "                        print(f\"Error on attempt {attempt + 1}, retrying in {delay}s...\")\n",
    "                        await asyncio.sleep(delay)\n",
    "                    else:\n",
    "                        return {\n",
    "                            \"success\": False,\n",
    "                            \"prompt\": prompt,\n",
    "                            \"error\": str(e)\n",
    "                        }\n",
    "    \n",
    "    async def batch_generate(self, prompts: List[str]) -> List[dict]:\n",
    "        \"\"\"Process multiple prompts concurrently.\"\"\"\n",
    "        print(f\"Processing {len(prompts)} prompts with max {self.semaphore._value} concurrent...\\n\")\n",
    "        \n",
    "        start = time.time()\n",
    "        results = await asyncio.gather(*[self.generate(p) for p in prompts])\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        successful = sum(1 for r in results if r[\"success\"])\n",
    "        \n",
    "        print(f\"\\nCompleted: {successful}/{len(prompts)} successful\")\n",
    "        print(f\"Time: {elapsed:.2f}s\")\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        print(f\"Cost estimate: ${self.total_tokens * 0.0000015:.4f} (GPT-3.5)\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage (requires API key)\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    processor = OpenAIBatchProcessor(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_concurrent=3\n",
    "    )\n",
    "    \n",
    "    prompts = [\n",
    "        \"Explain async programming in one sentence.\",\n",
    "        \"What is the benefit of concurrent requests?\",\n",
    "        \"How does rate limiting work?\",\n",
    "    ]\n",
    "    \n",
    "    results = await processor.batch_generate(prompts)\n",
    "    \n",
    "    for result in results:\n",
    "        if result[\"success\"]:\n",
    "            print(f\"\\nâœ“ {result['prompt']}\")\n",
    "            print(f\"  â†’ {result['response'][:80]}...\")\n",
    "else:\n",
    "    print(\"âš  Set OPENAI_API_KEY to run this example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices Summary\n",
    "\n",
    "### âœ… Always:\n",
    "\n",
    "1. **Use semaphores** to limit concurrency (5-20 for most APIs)\n",
    "2. **Set timeouts** on all operations (30s typical)\n",
    "3. **Implement retry logic** with exponential backoff\n",
    "4. **Track progress** for long operations\n",
    "5. **Handle errors individually** (don't fail entire batch)\n",
    "\n",
    "### ðŸŽ¯ For OpenAI API:\n",
    "\n",
    "- **Tier limits**: Check your account tier\n",
    "- **Rate limits**: 3-5 concurrent for most tiers\n",
    "- **Timeout**: 30-60s for chat completions\n",
    "- **Retry**: 3 attempts with 2s, 4s, 8s delays\n",
    "- **Cost tracking**: Count tokens, estimate costs\n",
    "\n",
    "### âš¡ Performance Tips:\n",
    "\n",
    "- Connection pooling: Use single `AsyncClient` instance\n",
    "- Batch size: 50-100 items per batch\n",
    "- Progress: Report every 10%\n",
    "- Logging: Log failures for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Patterns Covered:\n",
    "\n",
    "1. **Semaphores** - Control concurrency\n",
    "2. **Timeouts** - Prevent hanging\n",
    "3. **Retry + Backoff** - Handle transient failures\n",
    "4. **Progress tracking** - Monitor long operations\n",
    "5. **Production client** - Combine all patterns\n",
    "\n",
    "### Real Impact:\n",
    "\n",
    "These patterns enable:\n",
    "- Processing 1000s of prompts reliably\n",
    "- Respecting API rate limits\n",
    "- Handling failures gracefully\n",
    "- Building production-ready systems\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Exercise 1**: Build concurrent OpenAI client\n",
    "- **Exercise 2**: Implement rate-limited batch processor\n",
    "- **Module 02-02**: Type safety with Pydantic\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [asyncio semaphores](https://docs.python.org/3/library/asyncio-sync.html)\n",
    "- [OpenAI rate limits](https://platform.openai.com/docs/guides/rate-limits)\n",
    "- [Retry strategies](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
